In the previous chapter we showed how given the probability distribution of a random variable, we know everything about it and we can compute probabilities, expected values, and many more. In a way this is the job of descriptive statistics. In this chapter we will show how we can draw conclusions for the population, when the population is not accessible. This is the job of inferential statistics.

\section{Population VS Sample}

Let's begin with some basic definitions.

\bd[Population]
A \textbf{population} is a set of similar items or events which is of interest for some question or experiment. A statistical population can be a group of existing objects or a hypothetical and potentially infinite group of objects conceived as a generalization from experience.
\ed

Up to this point, technically we have been talking for populations. A population is the general category under examination. A r.v represents the population and the corresponding probability distribution tells us about the behaviour of the r.v and subsequently the behaviour of the population.

\bd[Parameter]
A \textbf{parameter} is a characteristic of a population.
\ed

Some examples of parameters is the expected value (or mean), the variance and the standard deviation of the population.\v

However, more often than not, the population is not available to us either because gathering data for all the population is very expensive or in most of the cases because it is impossible. For example, if we assume that our population is men's weights and we want to know about the mean parameter (i.e the mean weight of all men), weighting all men around the globe at the same time is impossible. Usually, we end up with a very small portion of the population called a \say{sample}. (To not be confused with \say{sample space})

\bd[Sample]
A \textbf{sample} is a subset of a population. The elements of a sample are known as sample points, sampling units or observations.
\ed

\vspace{-15pt}

\begin{figure}[H]
\includegraphics[scale=0.4]{images/population_and_sample.png}
\centering
\end{figure}

\vspace{-15pt}

Similarly to the definition of a parameter:

\bd[Statistic]
A \textbf{statistic} is a characteristic of a sample.
\ed

It follows from the definition of the sample that the latter is collected out of a population. In our previous example, one potential sample could be the weights of 1000 men.\v

Since samples are used to draw conclusion for the population one has to be extremely careful while collecting samples in order for the sample to be a good representative of the population. More specifically we must make sure that members of samples are randomly selected (each member of a population has an equal chance to be selected) and samples themselves are randomly selected (each sample of a population has an equal chance of being selected). Here are some definitions based on the way of collecting a sample.

\bd[Random Sampling]
\textbf{Random sampling} is a procedure for sampling from a population in which the selection of a sample unit is based on chance and every element of the population has a known, non-zero probability of being selected
\ed

\bd[Convenience Sampling]
\textbf{Convenience sampling} is a type of non - probability sampling that involves the sample being drawn from that part of the population that is close to hand.
\ed

\bd[Stratified Sampling]
\textbf{Stratified sampling} is a probability sampling technique that divides the entire population into different subgroups or strata, and then randomly selects the final subjects proportionally from the different strata.
\ed

In mathematical terms, given a probability distribution $P$, a random sample of length $n$ is a set of realizations of $n$ independent, identically distributed random variables (i.i.d r.v's) with distribution $P$. A sample concretely represents the results of $n$ experiments in which the same quantity is measured. In our example, if we want to estimate the mean weight of members of a particular population, we measure the heights of $n$ individuals. Each measurement is drawn from the probability distribution $P$ characterizing the population, so each measured weight $x_{i}$ is the realization of a r.v $X$ with distribution $P$. Hence mathematically a sample can be represented as $\{ x_{1}, x_{2}, \ldots, x_{n} \}$. Given this representation we can define some of the characteristics (statistics) of a sample.

\bd[Sample Size]
Given a sample of realizations of  of $n$ i.i.d r.v's $\{ x_{1}, x_{2}, \ldots, x_{n} \}$, we call \textbf{sample size} the direct count of the number of samples measured or observations being made $n$.
\ed

\bd[Sample Mean]
Given a sample of realizations of $n$ i.i.d r.v's $\{ x_{1}, x_{2}, \ldots, x_{n} \}$, we define the \textbf{sample mean} or average $\bar{x}$ of the sample as the quantity:
\bse
\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_{i}
\ese
\ed

\bd[Sample Variance]
Given a sample of realizations of $n$ i.i.d r.v's $\{ x_{1}, x_{2}, \ldots, x_{n} \}$, we define the \textbf{sample variance} $s^2$ of the sample as the quantity:
\bse
s^2 = \frac{1}{n} \sum_{i=1}^{n} (x_{i} - \bar{x})^2
\ese
\ed

\bd[Sample Standard Deviation]
Given a sample of realizations of $n$ i.i.d r.v's $\{ x_{1}, x_{2}, \ldots, x_{n} \}$, we define the \textbf{sample standard deviation} $s$ of the sample as the quantity:
\bse
s = \sqrt{s^2} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_{i} - \bar{x})^2}
\ese
\ed

\begin{theorem}[Law Of Large Numbers]
Given a random variable $X$ that follows a probability distribution $P$ with mean $\mu$ and a collection of $n$ realizations of $X$ $\{ x_{1}, x_{2}, \ldots, x_{n} \}$ with an average $\bar{x}_{n}$ then:
\bse
\bar{x}_{n} \to \mu \:\:\: for \:\:\: n \to \infty
\ese
\end{theorem}

The law of large numbers is a theorem that describes the result of performing the same experiment a large number of times. According to the law, the average of the results obtained from a large number of trials should be close to the actual expected value of the population, and will tend to become closer to the expected value as more trials are performed.The law of large numbers is important because it guarantees stable long-term results for the averages of some random events. 

\v

\begin{figure}[H]
\includegraphics[scale=0.25]{images/lawoflargenumbers.png}
\centering
\end{figure}

\v

The law of large numbers can find an application in statistics since, the \say{random variable $X$ that follows a probability distribution $P$ with mean $\mu$} can be translated to a population, and the \say{collection of $n$ realizations of $X$ $\{ x_{1}, x_{2}, \ldots, x_{n} \}$ with an average $\bar{x}_{n}$} can be translated to a sample of size  $n$ drawn out of the population. Then the law of large numbers simply states that as the size of the sample increases the sample mean tends to the actual population mean. Going back to our example, this means that as the number of men in our sample increases their average weight tends to the actual mean weight of the whole population.