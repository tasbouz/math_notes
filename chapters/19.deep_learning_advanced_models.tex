\section{Convolutional Neural Networks}

Convolutional neural network (CNN or ConvNet) is a class of deep neural networks, most commonly applied to analyse visual imagery.   The name ``convolutional" indicates that the network employs a mathematical operation called convolution.  As any deep learning model we've seen so far,  a CNN consists of an input layer,  hidden layers and an output layer.  The hidden layers include layers that, among others, perform convolutions.  

\vspace{5pt}

\begin{figure}[H]
\includegraphics[scale=0.3]{images/conv0.png}
\centering
\end{figure}

\vspace{5pt}

In what follows we will describe each step in the architecture of a CNN (figure above).  Since CNNs are used mainly to analyse images,  we will develop them by keeping in mind that we want to analyse an image. 

\subsection{Building A Convolutional Neural Network}

The first step is to find out a way for an image to be represented mathematically to an input that can be provided to the input layer of a neural network.  

\subsubsection*{Input Layer}

\bd[RGB Color Model]
The \textbf{RGB color model} is an additive color model in which red (R), green (G) and blue (B) light are added together in various ways to reproduce a broad array of colors.  The name of the model comes from the initials of the three additive primary colors,  red,  green, and blue. 
\ed

An image is a two dimensional collection of pixels where each pixel carries a specific shade of colour that can be decomposed to a specific combination of values for the RGB color model.  More precisely,  each pixel of a 2 dimensional image carries a position represented by ``row'' and ``column''.  (For example the very first pixel on the top left of an image is the pixel $(0,0)$).  We refer to the total number of rows of an image as the ``height'' of the image,  and to the total number of columns as the `width'' of the image.  Each position (pixel) then,  carries 3 values,  one for each color of RGB color model,  represented by a ``channel'',  where the combination of these values can represent the original image.  We refer to the total number of channels of an image as the ``depth'' of an image. (For the RGB color model the depth of any image is simply 3).

\begin{figure}[H]
\includegraphics[scale=0.32]{images/rgb.png}
\centering
\end{figure}

Hence,  a two dimensional image can be represented as a $(\textit{height} \times \textit{width} \times \textit{depth})$ dimensional matrix. Subsequently a collection of $N$ such images can be stack together to a $(N \times \textit{height} \times \textit{width} \times \textit{depth})$ dimensional matrix. This matrix is the mathematical representation of collection of images and serves as the input to a CNN. \v

The input layer of a CNN represents the input image.  Because we use RGB images as input,  the input layer has three channels,  corresponding to the red,  green,  and blue channels,  respectively,  which are shown in this layer. 

\subsubsection*{Convolutional Layer}

Now that we know how to represent an image to a matrix so we can feed it to a neural network, we can move on to the first building block of a CNN which is the so called ``convolutional" layer.  The convolutional layers are the foundation of CNNs, as they contain the learned kernels (weights), which extract features that distinguish different images from one another (this is what we want for classification)!  \v

Let's start by defining convolution in a strict, mathematical way.

\bd[Convolution]
\textbf{Convolution} is a mathematical operation on two functions $f$ and $g$ that produces a third function expressing how the shape of one is modified by the other. The term convolution refers to both the result function and to the process of computing it. It is defined as the integral of the product of the two functions after one is reversed and shifted.
\bse
(f*g)(t) = \int_{-\infty }^{\infty } f(\tau) g(t-\tau ) d\tau
\ese
\ed

In simple words convolution expresses how the shape of one function is modified by another function.   \v

While convolution as a term is very widely used in mathematics,  for now we will switch gears and examine convolution specifically for CNNs.  In order to do so we will give some very basic definitions in the world of CNNs.

\bd[Kernel/Filter/Mask]
A \textbf{kernel}, or \textbf{filter},  or \textbf{mask} is a small,  convolutional $f \times f$ (called size) matrix that is used to perform a convolution on a given image.
\ed

Here is an example of a kernel of size 3.

\vspace{-10pt}

\begin{figure}[H]
\includegraphics[scale=0.32]{images/conv.png}
\centering
\end{figure}

\vspace{-10pt}

Kernel size has a massive impact on the image classification task.  For example,  small kernel sizes are able to extract a much larger amount of information containing highly local features from the input.  Conversely, a large kernel size extracts less information, which leads to a faster reduction in layer dimensions,  often leading to worse performance.  Large kernels are better suited to extract features that are larger.  At the end of the day,  choosing an appropriate kernel size will be dependent on the task and dataset,  but generally,  smaller kernel sizes lead to better performance for the image classification task because an architecture designer is able to stack more and more layers together to learn more and more complex features.

\bd[Stride]
\textbf{Stride} is the number of pixels shifts over the input matrix (image) when the convolution is performed with a kernel. 
\ed

Stride indicates how many pixels the kernel should be shifted over at a time.  The impact stride has on a CNN is similar to kernel size.  As stride is decreased,  more features are learned because more data is extracted, which also leads to larger output layers.  On the contrary,  as stride is increased, this leads to more limited feature extraction and smaller output layer dimensions.  One responsibility of the architecture designer is to ensure that the kernel slides across the input symmetrically when implementing a CNN.  Use the hyperparameter visualization above to alter stride on various input/kernel dimensions to understand this constraint. \v

Now we have all the ingredients needed to perform convolution on an image.  More specifically,  given an input (image), a kernel and a stride we can perform the convolution by performing an an element-wise dot product between the kernel and the image and summing up the results.  We then move by one stride to the left and we continue up to the end.  Let's see this in action in a toy example.

\begin{figure}[H]
\includegraphics[scale=0.45]{images/con1.png}
\centering
\end{figure}

After the convolution the final result will be the following.

\begin{figure}[H]
\includegraphics[scale=0.45]{images/con2.png}
\centering
\end{figure}

The following figure shows step by step how the convolutions is done in a``real-world example" and what the final result is.

\begin{figure}[H]
\includegraphics[scale=0.45]{images/conv2.png}
\centering
\end{figure}

As it makes sense,  different kernels produce different results.  

\begin{figure}[H]
\includegraphics[scale=0.35]{images/con3.png}
\centering
\end{figure}

\begin{figure}[H]
\includegraphics[scale=0.35]{images/con4.png}
\centering
\end{figure}

\begin{figure}[H]
\includegraphics[scale=0.35]{images/con5.png}
\centering
\end{figure}

\bd[Padding]
The \textbf{padding} $p$ is the process of adding extra layers of empty pixels (all rgb values equal to 0) around an image.  We measure padding by the numbers of extra layers around the image (i.e $p=0$ means no extra layer, $p=1$ means one extra layer and so on).
\ed

Padding is often necessary when the kernel extends beyond the activation map.  In other words we use padding in order to avoid cases where the kernel does not fit the dimensions of an image and to avoid losing information from the corners of an image where the kernel (by definition) does not fit.  Here is an example of an image with padding $p=2$ (two layers).

\begin{figure}[H]
\includegraphics[scale=0.4]{images/conv3.png}
\centering
\end{figure}

Padding conserves data at the borders of activation maps,  which leads to better performance,  and it can help preserve the input's spatial size, which allows an architecture designer to build depper, higher performing networks.  The most common cases of padding are:
\bit
\item \textbf{Valid} padding where we have no padding at all ($p=0$)
\item \textbf{Same} padding where the output matrix is the same size as the input size.
\eit

Hence, the convolutional neuron performs an element-wise dot product with a unique kernel and the output of the previous layer's corresponding neuron.  This will yield as many intermediate results as there are unique kernels. The convolutional neuron is the result of all of the intermediate results summed together with the learned bias.  Subsequently the convolutional layer of a CNN performs many convolutions with different kernels (neurons).  \v

As we have already seen, the convolution changes (usually reduces) the dimensions of the original input. Just for completeness we will provide the formulas that calculate the output dimensions. More specifically,  given an original image of height $h$,  width $w$ and depth (channels) $d$,  after performing a convolutions with $K$ number of kernels of size $f$,  stride $s$ and padding $p$ we end up with the final result of height $h^\prime$,  width $w^\prime$ and depth (channels) $d^\prime$ given by:
\bse
h^\prime = \frac{h - f +2p}{s} +1, \qquad w^\prime = \frac{w - f +2p}{s} +1, \qquad d^\prime = K
\ese

After the convolution is done,  we feed the final result to an activation function (usually a ReLU).  This activation function is applied element-wise on every value from the input tensor after every convolutional layer in the network architecture.  (In simple words,  when the activation is a ReLU, we simply substitute all negative values of the result of the convolution with 0).  This process is also considered as part of the convolutional layer. \v

Now that we fully described the convolutional layer we move to the pooling layer.

\subsubsection*{Pooling Layer}

There are many types of pooling layers in different CNN architectures,  but they all have the purpose of gradually decreasing the spatial extent of the network, which reduces the parameters and overall computation of the network.  \v

The pooling operation requires selecting a kernel size and a stride length during architecture design. Once selected, the operation slides the kernel with the specified stride over the input while only selecting the corresponding pooling value at each kernel slice from the input to yield a value for the output.  For example a max-pooling layer would pick the maximum value of the kernel slice while an average-pooling would pick the average value of the kernel slice.

\begin{figure}[H]
\includegraphics[scale=0.35]{images/conv4.png}
\centering
\end{figure}

Pooling layers are used to reduce the dimensions of the feature maps.  Thus, they reduce the number of parameters to learn and the amount of computation performed in the network.  This has as a consequence for pooling layer to act as a form of regularization to the network. They carry no parameters themselves hence they do not contribute to the learning directly.  The pooling layers summarise the features present in a region of the feature map generated by a convolutional layer.  So, further operations are performed on summarised features instead of precisely positioned features generated by the convolutional layer.s  This makes the model more robust to variations in the position of the features in the input image. \v

In general in a CNN is typical to have a sequence of convolutional and pooling layers.  The more convolutional and pooling layers it contains the more deep it is.  As we will see,  different architectures use different combinations of these layers.

\subsubsection*{Flatten Layer}

After all the convolutional and pooling layers are done, the final result is feed to a so-called ``flatten" layer. This layer converts a three-dimensional layer in the network into a one-dimensional vector to fit the input of a fully-connected layer for classification.  For example, a $(5 \times 5 \times 2)$ matrix would be converted into a vector of size 50. The previous convolutional and pooling layers of the network extracted the features from the input image,  but now it is time to classify the features. 

\v

\begin{figure}[H]
\includegraphics[scale=0.48]{images/conv5.png}
\centering
\end{figure}

This layer is purely a ``helper" layer that transforms the data in order to fit the needs of the input of a feedforward neural network.  It carries no parameters hence it does not contribute to the learning. 

\subsubsection*{Feedforward Neural Network}

After the flatten layer the image is now in a form that can be fed to a feedforward neural network (i.e a classifier) and with this final component the CNN architecture is built and it's finally ready to make predictions.  We have already spent a whole chapter talking about feedforward neural networks, and we already know that they carry some parameters that we need to learn in order for the model to be trained.  However,  on top of that,  instead of using hand-crafted kernels such as edge detectors in the convolutional layers, we can let the model to learn meaningful kernels/filters in addition to learning the weights of the classifier (remember that pooling layers and flatten layer do not carry any parameters). In other words simply by treating these kernels as parameters and learning them in addition to the weights of the classifier (using back propagation) we can have a ``new" more complex neural network called a ``convolutional neural network".  

\subsection{Convolutional Neural Network Architectures}

As it is clear by now,  there is an infinite amount of possible CNN architectures based on the combinations of number of convolutional and pooling layers in the network and the parameters of the final feedforward neural network. On top of that we have also an uncountable number of options for hyperparameters such as filter size, stride, and padding for each of the convolutional layers.  Long story short, the variety of possible CNNs is endless. It does not come as a surprise thus,  that there are some heavily researched and widely used architectures that are used regularly in deep learning.  in this section we will (very quickly) mention some of them!

\subsubsection{LeNet}

LeNet is a convolutional neural network structure proposed by Yann LeCun in 1989. In general,  LeNet refers to LeNet-5 and is a simple convolutional neural network. 

\begin{figure}[H]
\includegraphics[scale=0.6]{images/conv6.png}
\centering
\end{figure}

\subsubsection{AlexNet}

AlexNet is the name of a convolutional neural network architecture competed in the ImageNet Large Scale Visual Recognition Challenge, designed by Alex Krizhevsky, Ilya Sutskever and Geoffrey Hinton.

\begin{figure}[H]
\includegraphics[scale=0.3]{images/conv7.png}
\centering
\end{figure}

\subsubsection{ZFNet}

ZFNet is a classic convolutional neural network very similar to AlexNet. Compared to AlexNet,  the filter sizes are reduced and the stride of the convolutions are reduced.  In the diagram we can see in red numbers the differences in the parameters with AlexNet. 

\v

\begin{figure}[H]
\includegraphics[scale=0.35]{images/conv8.png}
\centering
\end{figure}

\subsubsection{VGGNet}

The VGG network architecture was introduced by Simonyan and Zisserman in their 2014 paper,  Very Deep Convolutional Networks for Large Scale Image Recognition. This network is characterized by its simplicity,  using only $3 \times 3$ convolutional layers stacked on top of each other in increasing depth. 

\begin{figure}[H]
\includegraphics[scale=0.35]{images/conv9.png}
\centering
\end{figure}

\subsubsection{GoogLeNet}

GoogLeNet is a 22 layer deep convolutional neural network that is a variant of the Inception Network,  a Deep Convolutional Neural Network developed by researchers at Google. The GoogLeNet architecture presented in the ImageNet Large Scale Visual Recognition Challenge solved computer vision tasks such as image classification and object detection.

\begin{figure}[H]
\includegraphics[scale=0.35]{images/conv10.png}
\centering
\end{figure}

\section{Word Embedding}

In natural language processing (NLP),  word embedding is a term used for the vectorial representation of words for text analysis,  typically in the form of a real valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning.  Word embeddings can be obtained using a set of language modelling and feature learning techniques where words or phrases from the vocabulary are mapped to vectors of real numbers.  Conceptually it involves a mathematical embedding from a space with many dimensions per word to a continuous vector space with a much lower dimension. \v

Let us start with a very simple motivation for why we are interested in a vectorial representations of words. Suppose we are given an input stream of words like a sentence or a document and we are interested in learning some function of it e.g: $\hat{y} = \textit{sentiments(words)}$.  Say,  we employ a machine learning algorithm (some mathematical model) for learning such a function $\hat{y} = f(x)$. We first need a way of converting the input stream (or each word in the stream) to a vector.  Let's start with two important definitions.\v

\bd[Corpus]
A \textbf{corpus} is a language resource consisting of a large and structured set of texts. 
\ed

\bd[Vocabulary]
Given a corpus, a \textbf{vocabulary} $V$ is the set of all unique words in the corpus.
\ed

There are two big classes of methods of word embedding: the ``count based models" which they use the co-occurrence counts of words, and the ``prediction based models" which directly learn word representations.  The first class of count based models were the one used before the discovery of deep learning while prediction based models are the latest advancements in the area of word embedding that make use of deep learning. We will begin by briefly introducing the count based models, since they will be useful for later, and then we switch to the main part of prediction based models. 

\subsection{One-Hot Representation}

Word embedding is actually the attempt of a vector representation for every word in a vocabulary $V$.  The simplest way to do so is the so called ``one-hot'' representation.

\bd[One-Hot Representation]
One-hot representation is a group of bits among which the legal combinations of values are only those with a single high 1 bit and all the others low 0.  A similar implementation in which all bits are 1 except one 0 is sometimes called `one-cold''. 
\ed

The following is an example of a one-hot representation of the vocabulary $V = [$ ``cat'',  ``dog'', ``truck'' $]$:

\begin{figure}[H]
\includegraphics[scale=0.4]{images/onehot.png}
\centering
\end{figure}

While one-hot representation is very simple and intuitive, it carries a lot of problems. First of all,  usually vocabularies tend to be very large hence the vector space turns enormous which is computationally very inefficient.  Second,  the representation is very sparse, meaning that the the vast majority of the entries are zeros. Third, and most important,  the representation does not capture any notion of similarity among the words. In our example,  ideally,  we would want the representations of cat and dog to be closer to each other
than the representations of cat and truck.  However, with one-hot representation, the Euclidean distance between any
two words in the vocabulary is simply $\sqrt{2}$.  Similarly the cosine similarity between any two words in the vocabulary is
$0$.

\subsection{Distributed Representation}

We will now introduce the distributional similarity based representation. The idea behind distributed representation is expressed in the phrase ``you shall know a word by the company it keeps''. 

\bd[Co-Occurrence Matrix]
Given a vocabulary $V$ with $n$ terms, the \textbf{co-occurrence matrix} is an $(n \times n)$ matrix which captures the number of times a term appears in the context of another term,  where context is defined as a window of $k$ words around the terms.  Each row (or column) of the co-occurrence matrix gives a vectorial representation of the corresponding word.
\ed

As an example,  let us build a co-occurrence matrix for a toy corpus with $k = 2$.  The corpus consist of the following sentences:
\bit
\item Human machine interface for computer applications.
\item User opinion of computer system response
time.
\item User interface management system.
\item System engineering for improved response
time.
\item System engineers optimize for human experience.
\eit

The vocabulary looks like this: $V = [$ ``Human",  ``Machine",  ``Interface", ``For",  ...  $]$.  Finally,  the co-occurrence matrix looks like this:

\begin{figure}[H]
\includegraphics[scale=0.65]{images/cooccurance.png}
\centering
\end{figure}

Of course,  as with one-hot,  also this representation carries its own problems.  First of all, stop words like (``a",  ``the", `` for",  ...) are very frequent hence these counts will be very high.  This however,  is easily solvable since we can either ignore very frequent words or simply set a threshold $t$ (for example $t=100$) and whenever the count of a word gets more that $t$ we simply stop counting further and we use the threshold $t$ as the value,  ignoring all the other occurrences.  \v

The most important problems of distributed representation though are that,  as in hot-one representation,  it is very high dimensional, it is very sparse and it grows with the size of the vocabulary.  All of these problems were solved by using singular value decomposition as a dimensionality reduction technique.  However,  we will not see that in more detail since it is not related to deep learning and it gets out of topic. \v

For many years the singular value decomposition of distributed representation was the way to go for NLP and word embedding problems.  However,  in 2006 and after,  after the discovery of deep learning,  we switched gears from the whole idea of counting word occurrences and co-occurrence matrices of count based models to direct,  prediction based models that uses statistics to predict outcomes.  In what follows we will introduce the most heavily used group of techniques of prediction based models called ``Word2vec''.

\subsection{Word2vec}

Word2vec is a technique for natural language processing that was created,  patented,  and published in 2013 by a team of researchers led by Tomas Mikolov at Google over two papers.  The word2vec algorithm uses a neural network model to learn word associations from a large corpus.  Once trained,  such a model can detect synonymous words or suggest additional words for a partial sentence.  As the name implies,  word2vec represents each distinct word with a particular list of numbers called a vector. The vectors are chosen carefully such that a simple mathematical function (the cosine similarity between the vectors) indicates the level of semantic similarity between the words represented by those vectors.  Embedding vectors created using the word2vec algorithm have some advantages compared to earlier algorithms that we described previously.\v

Word2vec is a group of related models that are used to produce word embeddings. These models are shallow,  two-layer neural networks that are trained to reconstruct linguistic contexts of words.  Word2vec takes as its input a large corpus of text and produces a vector space,  typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space.  Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located close to one another in the space. \v

As we mentioned, word2vec can utilize either of two model architectures to produce a distributed representation of words: ``continuous bag-of-words'' (CBOW) or ``continuous skip-gram''.   In the continuous bag-of-words architecture, the model predicts the current word from a window of surrounding context words. The order of context words does not influence prediction (bag-of-words assumption).  In the continuous skip-gram architecture,  the model uses the current word to predict the surrounding window of context words. The skip-gram architecture weighs nearby context words more heavily than more distant context words. According to the authors' note CBOW is faster while skip-gram does a better job for infrequent words. In what follows we will introduce both of them since they are the standard way of dealing with word embeddings today.

\subsubsection{Continuous Bag-Of-Words (CBOW)}

The Continuous Bag-Of-Words (CBOW) model tries to understand the context of the words and takes this as input.  It then tries to predict words that are contextually accurate.  In other words it tries to predict the current target word based on the source context words (surrounding words).  Let's see an example.  Consider the simple sentence ``the man sat on a chair". This can be seen as pairs of (context,  word) where if we consider a context window of size 1 (for simplicity) we have ([the],  man),  ([man],  sat),  ([sat],  on), ([on],  a),  ([a],  chair).  We have used a list for context just to make obvious that if one picks a window greater than 1 then we would have more context words, e.g for a window of size 2 we would examples like (the, man], sat), ([man, sat], on) and so on...  Thus the model tries to predict the target word based on the context word.  \v

The CBOW model solves this problem by using a feedforward neural network.  The input to the network will be a one-hot representation of the context words and the output,  will be a probability distribution over all possible words in the vocabulary (multi-class classification problem).  Let's see this in our simple example with the window of size 1.

\vspace{10pt}

\begin{figure}[H]
\includegraphics[scale=0.65]{images/cbow.png}
\centering
\end{figure}

In this particular example the input layer to the network is a one-hot representation of the context word ``sat" which is: 
\bse
\textit{``sat"} = [0,1,0,0, \ldots, 0]
\ese

\v

The network is shallow since it contains only one single hidden layer.  The first set of weights $W^{[1]}$ are of course of dimensions $(k \times |V|)$ where $k$ is the number of units in the hidden layer and $|V|$ is the size of the vocabulary which is of course also the size of the one-hot representation vectors.  It is common practice to call $W^{[1]}$ simply $W_{\textit{context}}$.  Notice that we carry no biases.  The pre-activations are:
\bse
\textbf{a}^{[1]} =W_{\textit{context}}  \cdot \textbf{x}
\ese

and the activation function $g$ is as simple as  $g(\textbf{x}) = \textbf{x}$, hence the activation of the hidden layer $\textbf{h}^{[1]}$ is:
\bse
\textbf{h}^{[1]} =W_{\textit{context}}  \cdot \textbf{x}
\ese

Notice that the product $W_{\textit{context}} \cdot \textbf{x}$, given that $\textbf{x}$ is a one hot-vector,  is simply the $\textit{i}^{\textit{th}}$ column of $W_{\textit{context}}$  since everything else will be multiplied by 0.  So when the $\textit{i}^{\textit{th}}$  word is present the $\textit{i}^{\textit{th}}$ element in the one-hot vector is on and the $\textit{i}^{\textit{th}}$  column of $W_{\textit{context}}$ gets selected. In other words,  there is a one-to-one correspondence between the words and the column of $W_{\textit{context}}$. More specifically, we can treat the $\textit{i}^{\textit{th}}$  column of $W_{\textit{context}}$ as the representation of the $\textit{i}^{\textit{th}}$ context.

\begin{figure}[H]
\includegraphics[scale=0.4]{images/cbow2.png}
\centering
\end{figure}

Moving on, the weights of the output layer $W^{[2]}$ are of course of dimensions $(|V| \times k)$,  since we need an output (aka a probability) for each possible word in the vocabulary (aka a probability distribution).  It is common practice to call $W^{[2]}$ simply $W_{\textit{word}}$.  The pre-activations now are:
\bse
\textbf{a}^{[2]} =W_{\textit{word}}  \cdot \textbf{h}^{[1]}
\ese

and for the output function $O$ it makes sense to use a softmax since we are dealing with a multi-class classification problem,  hence:
\bse
\hat{\textbf{y}} = P(\textit{word \:} | \textit{\: context}) = \frac{e^{W_{\textit{word}}  \cdot \textbf{h}^{[1]}}}{\sum_{\textit{words}} e^{W_{\textit{word}}  \cdot \textbf{h}^{[1]}}}
\ese

As we can see,  since:
\bse
W_{\textit{word}}  \cdot \textbf{h}^{[1]} = W_{\textit{word}}  \cdot W_{\textit{context}}  \cdot \textbf{x}
\ese

\v

and since from the product $W_{\textit{context}}  \cdot \textbf{x} \:$ survives only the column of $W_{\textit{context}}$ associated with the context,  it turns out that $P(\textit{word \:} | \textit{\: context})$ is proportional to the dot product between of $W_{\textit{context}}$ and $W_{\textit{word}}$. In other words the probability for each word depends on the $\textit{i}^{\textit{th}}$ column of $W_{\textit{word}}$.  We thus treat the $\textit{i}^{\textit{th}}$ column of $W_{\textit{word}}$ as the representation of the $\textit{i}^{\textit{th}}$ word. \v

Now the forward propagation is done and the network is built.  In order to train it we move to backpropagation.  First of all, we need to calculate the loss function.  Since we are dealing with a multi-class classification problem we will use the cross entropy loss function.  We have already calculated the cross-entropy loss function for logistic regression. The calculations for a softmax regression are very similar so we will skip them.  Just for completeness we will use the principle of maximum likelihood to obtain the cross entropy loss function for softmax which simply is:
\bse
\mathcal{L}(\theta) = - \ln P(\textit{word \:} | \textit{\: context}) = - \ln \frac{e^{W_{\textit{word}}  \cdot \textbf{h}^{[1]}}}{\sum_{\textit{words}} e^{W_{\textit{word}}  \cdot \textbf{h}^{[1]}}}
\ese

From here we just follow the backpropagation theory we have already developed, i.e we calculate the update rules using the chain rule,  and we simply train the model through iteration of passing data. Notice however, that the softmax function at the output is computationally very expensive since the denominator requires a summation over all words in the vocabulary.  We will revisit this issue soon.

\subsubsection{Continuous Skip-Gram}

The Skip-Gram model architecture tries to achieve the reverse of what the CBOW model does.  It tries to predict the context words (surrounding words) given a target word (the center word). Considering our simple sentence from earlier,  ``the man sat on a chair" in the CBOW model,  we get pairs of (context, word) where if we consider a context window of size 1,  we have examples like ([the],  man),  ([man],  sat),  ([sat],  on) and so on.  Now considering that the skip-gram model's aim is to predict the context from the word,  the model typically inverts the contexts and words,  and tries to predict each context word from its target word.  Hence the task becomes to predict the context [the] given target word ``man" and so on. Thus the model tries to predict the context window words based on the target word. \v

Notice that in the case of where the context window is equal to 1, the two models are in practice identical since both of them try to predict one word based on another word.  However in the case where the window is greater than 1, then CBOW tries to predict one word based on all the context words, while skip-gram tried to predict all the context words based on one word.  This is why we said that skip-gram is actually the reverse of CBOW. 

\begin{figure}[H]
\includegraphics[scale=0.65]{images/skipgram.png}
\centering
\end{figure}

Just like we discussed in the CBOW model,  we need to model this skip-gram architecture now as a deep learning classification model such that we take in the target word as our input and try to predict the context words.  The architecture is similar to the CBOW model with the difference that now everything is reversed.  The input is again just one word (e.g the word ``on") with one-hot representation:
\bse
\textit{``on"} = [0,0,1,0, \ldots, 0]
\ese

but now the name of the weights are reversed since their meaning is reversed (see figure).  So far only the naming has changed.  The mathematical difference is out the outcome, where now instead of just on word we want to predict all the context words.  We do that by having multiple predictions,  hence multiple softmax output function and subsequently the loss function is simply the sum of all individual cross-entropy loss functions:
\bse
\mathcal{L}(\theta) = - \sum_{\textit{words in window}} \ln P(\textit{word \:} | \textit{\: context})
\ese

From this point on everything is exactly the same. We train the model through backpropagation simply by computing the update rules and running training iterations.  Notice that the same problem as with CBOW is also present here , since the softmax function at the output is computationally very expensive since the denominator requires a summation over all words in the vocabulary.  \v

There are 3 popular ways to overcome this problem.  We will just mention them for now, but we will not explain them:
\bit
\item Solution 1: Negative sampling.
\item Solution 2: Contrastive estimation.
\item Solution 3: Hierarchical softmax.
\eit

Finally it is worth mentioning the existence of a model that combines SVD and word2vec called ``GloVe''. 

\section{Recurrent Neural Networks}

Up to this point in all feedforward and convolutional neural networks we have developed,  the size of the input was always fixed.  For example, we fed fixed size images to convolutional neural networks for image classification and fixed sized window of words in word2vec models. Further,  each input to the network was independent of the previous or
future inputs.  For example, the computations,  outputs and decisions for two successive images are completely independent of each other,  and each word prediction does not depend in the previous prediction.  \v

However,  in many applications the input is not of a fixed size.  On top of that, successive inputs may not be independent of each other.  For example,  consider the task of auto completion.  Given the first character ``d" you want to predict the next character ``e" and
so on...  In this example successive inputs are no longer
independent (while predicting ``e" you would want to know what the previous input was in addition to the current input) and the length of the inputs and the number of predictions you need to make is not fixed (for example,  ``learn",  ``deep",  ``machine" have different number of characters).   

\begin{figure}[H]
\includegraphics[scale=0.6]{images/rnn0.png}
\centering
\end{figure}

This kind of problems are known as ``sequence learning
problems",  because we need to look at a sequence of (dependent) inputs and produce an output (or outputs).  Let us look at some more examples of such problems.  \v

Consider the task of predicting the part of speech tag (noun, adverb, adjective, verb) of each word in a sentence.  Once we see an adjective (social) we are almost sure that the next word should be a noun (man).
Thus the current output (noun) depends on the current input as well as the previous input.  Furthermore,  the size of the input is not fixed (sentences could have arbitrary number of words).  Notice that here we are interested in producing an output at each time step and that each network is performing the same task (input : word, output : tag).

\begin{figure}[H]
\includegraphics[scale=0.6]{images/rnn.png}
\centering
\end{figure}

Notice that in this example we are interested in producing an output at each time step.  We usually call this sequential problem as ``many to many" since we get many inputs and we provide many outputs. \v

Sometimes we may not be interested in producing an output at every stage.  Instead we would look at the full sequence and then produce an output.  For example, consider the task of predicting the polarity of a movie review.  The prediction clearly does not depend only on the last word but also on some words which appear before. Here again we could think that the network is performing the same task at each step (input : word, output : tag) but it is just that we don't care about intermediate outputs. We usually call this sequential problem as ``many to one" since we get many inputs and we provide only one output.

\vspace{-10pt}

\begin{figure}[H]
\includegraphics[scale=0.6]{images/rnn2.png}
\centering
\end{figure}

\vspace{-10pt}

Of course,  sequences could be composed of anything (not just words).
For example,  a video could be treated as a sequence of images,  audio as a sequence of wavelengths and time series as a sequence of data. \v

Depending on the nature of the sequential problem we might end up having different kind of problems. On top of ``many to many" and ``many to one" one can also have ``one to one",  ``one to many",  and a different kind of ``many to many". The following figure summarizes all the different kind of sequential problems.

\begin{figure}[H]
\includegraphics[scale=0.85]{images/rnn00.png}
\centering
\end{figure}

No matter the nature of the data or the category of the problem we need to develop a model able to deal with these kind of problems. 

\subsection{Building A Recurrent Neural Network}

A recurrent neural network (RNN) is a class of artificial neural networks where connections between units form a directed cycle.  This creates an internal state of the network which allows it to exhibit dynamic temporal behaviour.  Unlike feedforward neural networks,  RNNs can use their internal memory to process arbitrary sequences of inputs. This makes them applicable to sequential tasks.  Let's try to build an RNN one step at a time. \v

Recall that we wanted to solve the following problems:
\bit
\item Account for variable number of inputs.
\item Account for dependence between inputs.
\eit

Solving the first problem is easy.  Our neural network has to be flexible to the input.  The idea here is instead of having just one neural network,  to have a lot of them and feed each unit of input to each one of them (like the examples we saw above). 

\begin{figure}[H]
\includegraphics[scale=0.6]{images/rnn3.png}
\centering
\end{figure}

In the diagram above,  the  orange box represents the input layer, the blue box represents a generic deep feedforward neural network (as a black box) and the green box represents the output layer.  The matrix $U$ represents the input layer parameters (both weights and biases) and the matrix $V$ represent the output layer parameters (again both weights and biases).  Once again the symbols for the various parameters are different for RNNs,  but nothing really changes in reality. \v

The crucial point here is that we need to make sure that the function executed at each time step (neural network) is the same.  In the case were the function being computed at each time-step now was different:
\bse
\hat{y}_1 = f_1(\boldsymbol{x_1}), \qquad \hat{y}_2 = f_2(\boldsymbol{x_1}, \boldsymbol{x_2}), \qquad \hat{y}_3 = f_3(\boldsymbol{x_1},\boldsymbol{x_2},\boldsymbol{x_3})
\ese

then the network would be sensitive to the length of the sequence since it would require $f_1, f_2, \ldots,$ functions without us knowing what the number of them should be or with no requirement to stay constant.  \v

Hence,  since we want the same function to be executed at each time-step we should share the same network (i.e., same parameters at each time-step). This parameter sharing ensures that the network becomes agnostic to the length (size) of the input since we are simply going to compute the same function (with same parameters) at each time-step,  hence the number of time-steps doesn't matter.  In other words we just create multiple copies of the network and execute them at each time-step.  \v

Coming to the second issue, having multiple copies of the same network, does not solve the fact the the inputs are dependent to each other. In order to deal with this problem,  we will add a recurrent
connection in the network as it is shown in the figure below.

\begin{figure}[H]
\includegraphics[scale=0.6]{images/rnn4.png}
\centering
\end{figure}

where $\boldsymbol{s_i} $ is a $(d \times 1)$ dimensional vector called the ``state of the network at time-step $i$" given by:
\bse
\boldsymbol{s_i} = g(U \boldsymbol{x_i} + W \boldsymbol{s_{i-1}})
\ese

with $W$ being a new $(d \times d)$ matrix of parameters (not part of usual feedforward neural networks) used as weights for the states.   \v

As a reminder, the input vector $\boldsymbol{x}$ is a $(n \times 1)$ dimensional vector and the weight matrix $U$ is a $(d\times n)$  dimensional matrix.  Observe that,  since the biases are integrated in the weights,  the term $U \boldsymbol{x_i}$ is actually the usual activation: 
\bse
\boldsymbol{a}(\boldsymbol{x_i} ; U) = U \boldsymbol{x_i}
\ese

\v

we had in the feedforward neural networks. The extra term $W \boldsymbol{s_{i-1}}$ in the activation is what gives the dependency between the inputs.  Finally the function $g$, as in feedforward neural networks, is the one responsible for the non-linearity.  \v

Of course for the targets of the network still holds:
\bse
\hat{y}_i = O(V \boldsymbol{s_i})
\ese

where $O$ is the output layer function depending on the nature of the problem under investigation.  \v

It is very important to realize that the parameters $W,  U,  V$ are the same across all time-steps in order to make sure that the network is independent of the input size.  And this is the basic architecture of a recurrent neural network.   \v

For convenience instead of drawing multiple networks in succession we usually draw an RNN as follows.

\vspace{-5pt}

\begin{figure}[H]
\includegraphics[scale=0.6]{images/rnn5.png}
\centering
\end{figure}

\subsection{Backpropagation Through Time}

As with any other neural network we have developed so far, once the network is built then we need to train it.  Similarly we will do for the RNN.  First of all,  we need a loss function.  Suppose we initialize all parameters $\theta = \{U, V, W\}$ randomly and the network predicts the targets $\hat{y}_i$.  Each network carries (the same) loss $J$ hence we have $n$ losses of the form $J(y_i,  \hat{y}_i ; \theta)$ for each of the $n$ copies of the sub-networks.  The total loss is simply the sum of the loss over all time-steps:
\bse
J(\theta) = \sum_{i=1}^{n} J(y_i,  \hat{y}_i ; \theta)
\ese

Now that we have a loss, we need an algorithm to train the model.  Unsurprisingly,  RNNs are trained with backpropagation, with the only difference that now we need to update all parameters $\theta = \{U, V, W\}$ during backpropagation. For the updates we need to compute the gradients with respect to all parameters $\theta$ in order to perform the updates in gradient descent.  Since the derivative is linear,  for any parameter $\theta$ holds:
\bse
\nabla_\theta J(\theta) = \sum_{i=1}^{n} \nabla_\theta J(y_i,  \hat{y}_i ; \theta)
\ese

So it suffices to compute $\nabla_\theta J(y_i,  \hat{y}_i ; \theta)$. \v

Starting with $V$ the derivation is exactly the same as in the usual backpropagation:
{\setlength{\jot}{5pt}
\begin{align*}
\nabla_V J(y_i,  \hat{y}_i ; \theta)& = \frac{\partial J}{\partial \hat{y}_i} \cdot \frac{\partial \hat{y}_i}{\partial (V \boldsymbol{s_i})} \cdot \frac{\partial (V \boldsymbol{s_i})}{\partial V} \\
& = \frac{\partial J}{\partial \hat{y}_i} \cdot \frac{ \partial ( O(V \boldsymbol{s_i}))}{\partial  (V \boldsymbol{s_i})} \cdot \boldsymbol{s_i}\\
& = \frac{\partial J}{\partial \hat{y}_i} \cdot O^\prime(V \boldsymbol{s_i}) \cdot \boldsymbol{s_i}
\end{align*}}

Moving on to $W$,  things get a bit different than the usual backpropagation due to the recurrent dependency of the network to W.  More precisely:
{\setlength{\jot}{5pt}
\begin{align*}
\nabla_W J(y_i,  \hat{y}_i ; \theta)& = \frac{\partial J}{\partial \hat{y}_i} \cdot \frac{\partial \hat{y}_i}{\partial (V \boldsymbol{s_i})} \cdot \frac{\partial (V \boldsymbol{s_i})}{\partial W} \\
&= \frac{\partial J}{\partial \hat{y}_i} \cdot \frac{ \partial ( O(V \boldsymbol{s_i}))}{\partial  (V \boldsymbol{s_i})} \cdot V \cdot \frac{\partial (\boldsymbol{s_{i}})} {\partial W} \\
&= \frac{\partial J}{\partial \hat{y}_i} \cdot O^\prime(V \boldsymbol{s_i}) \cdot V \cdot \frac{\partial (g(U \boldsymbol{x_i} + W \boldsymbol{s_{i-1}}))} {\partial W} \\
&=  \frac{\partial J}{\partial \hat{y}_i} \cdot O^\prime(V \boldsymbol{s_i}) \cdot V \cdot g^\prime(U \boldsymbol{x_i} + W \boldsymbol{s_{i-1}}) \cdot \frac{\partial (U \boldsymbol{x_i} + W \boldsymbol{s_{i-1}})} {\partial W} \\
& =  \frac{\partial J}{\partial \hat{y}_i} \cdot O^\prime(V \boldsymbol{s_i}) \cdot V \cdot g^\prime(U \boldsymbol{x_i} + W \boldsymbol{s_{i-1}}) \cdot \frac{\partial (W \boldsymbol{s_{i-1}})} {\partial W} \\
& =  \frac{\partial J}{\partial \hat{y}_i} \cdot O^\prime(V \boldsymbol{s_i}) \cdot V \cdot g^\prime(U \boldsymbol{x_i} + W \boldsymbol{s_{i-1}}) \cdot  \left[ \boldsymbol{s_{i-1}} + W \cdot \frac{\partial (\boldsymbol{s_{i-1}})} {\partial W} \right]
\end{align*}}

However $\boldsymbol{s_{i-1}}$ is also a function of $W$ since:
\bse
\boldsymbol{s_{i-1}}= g(U \boldsymbol{x_{i-1}} + W \boldsymbol{s_{i-2}})
\ese

By substituting back we obtain:
{\setlength{\jot}{5pt}
\begin{align*}
\nabla_W J(y_i,  \hat{y}_i ; \theta) & =  \frac{\partial J}{\partial \hat{y}_i} \cdot O^\prime(V \boldsymbol{s_i}) \cdot V \cdot g^\prime(U \boldsymbol{x_i} + W \boldsymbol{s_{i-1}}) \cdot  \left[ \boldsymbol{s_{i-1}} + W \cdot \frac{\partial (g(U \boldsymbol{x_{i-1}} + W \boldsymbol{s_{i-2}}))} {\partial W} \right] \\
& =  \frac{\partial J}{\partial \hat{y}_i} \cdot O^\prime(V \boldsymbol{s_i}) \cdot V \cdot g^\prime(U \boldsymbol{x_i} + W \boldsymbol{s_{i-1}}) \cdot  \left[ \boldsymbol{s_{i-1}} + W \cdot g^\prime (U \boldsymbol{x_{i-1}} + W \boldsymbol{s_{i-2}}) \cdot \frac{\partial (U \boldsymbol{x_{i-1}} + W \boldsymbol{s_{i-2}})} {\partial W} \right] \\
&= \frac{\partial J}{\partial \hat{y}_i} \cdot O^\prime(V \boldsymbol{s_i}) \cdot V \cdot g^\prime(U \boldsymbol{x_i} + W \boldsymbol{s_{i-1}}) \cdot  \left[ \boldsymbol{s_{i-1}} + W \cdot g^\prime (U \boldsymbol{x_{i-1}} + W \boldsymbol{s_{i-2}}) \cdot \frac{\partial (W \boldsymbol{s_{i-2}}))} {\partial W} \right] \\
&= \frac{\partial J}{\partial \hat{y}_i} \cdot O^\prime(V \boldsymbol{s_i}) \cdot V \cdot g^\prime(U \boldsymbol{x_i} + W \boldsymbol{s_{i-1}}) \cdot  \left[ \boldsymbol{s_{i-1}} + W \cdot g^\prime (U \boldsymbol{x_{i-1}} + W \boldsymbol{s_{i-2}}) \cdot \left[ \boldsymbol{s_{i-2}} + W \cdot \frac{\partial (\boldsymbol{s_{i-2}})} {\partial W} \right] \right] \\
&= \ldots
\end{align*}}

Similarly $\boldsymbol{s_{i-1}}$ is also a function of $W$:
\bse
\boldsymbol{s_{i-2}}= g(U \boldsymbol{x_{i-2}} + W \boldsymbol{s_{i-3}})
\ese

We substitute again back to the equation and with this recursive way we proceed until we reach the first network where the backpropagation is over.  The main difference with the usual backpropagation is that in RNNs beside of just backpropagating back to the network we also backpropagate over all previous time-steps (i.e back in time).  For this reason we call this backpropagation method ``backpropagation through time".  \v

Finally for $\nabla_U J(y_i,  \hat{y}_i ; \theta)$ we have exactly the same situation as with $W$,  i.e:

\begingroup
\allowdisplaybreaks
{\setlength{\jot}{5pt}
\begin{align*}
\nabla_U J(y_i,  \hat{y}_i ; \theta)& = \frac{\partial J}{\partial \hat{y}_i} \cdot \frac{\partial \hat{y}_i}{\partial (V \boldsymbol{s_i})} \cdot \frac{\partial (V \boldsymbol{s_i})}{\partial U} \\
&= \frac{\partial J}{\partial \hat{y}_i} \cdot \frac{ \partial ( O(V \boldsymbol{s_i}))}{\partial  (V \boldsymbol{s_i})} \cdot V \cdot \frac{\partial (\boldsymbol{s_i})}{\partial U} \\
&= \frac{\partial J}{\partial \hat{y}_i} \cdot O^\prime(V \boldsymbol{s_i}) \cdot V \cdot \frac{\partial (g(U \boldsymbol{x_i} + W \boldsymbol{s_{i-1}}))} {\partial U} \\
&=  \frac{\partial J}{\partial \hat{y}_i} \cdot O^\prime(V \boldsymbol{s_i}) \cdot V \cdot g^\prime(U \boldsymbol{x_i} + W \boldsymbol{s_{i-1}}) \cdot \frac{\partial (U \boldsymbol{x_i} + W \boldsymbol{s_{i-1}})} {\partial U} \\
& =  \frac{\partial J}{\partial \hat{y}_i} \cdot O^\prime(V \boldsymbol{s_i}) \cdot V \cdot g^\prime(U \boldsymbol{x_i} + W \boldsymbol{s_{i-1}}) \cdot  \left[ \boldsymbol{x_{i}} + W \cdot \frac{\partial (\boldsymbol{s_{i-1}})} {\partial U} \right] \\
& =  \frac{\partial J}{\partial \hat{y}_i} \cdot O^\prime(V \boldsymbol{s_i}) \cdot V \cdot g^\prime(U \boldsymbol{x_i} + W \boldsymbol{s_{i-1}}) \cdot  \left[ \boldsymbol{x_{i}} + W \cdot \frac{\partial (g(U \boldsymbol{x_{i-1}} + W \boldsymbol{s_{i-2}}))} {\partial U}\right] \\
& =  \frac{\partial J}{\partial \hat{y}_i} \cdot O^\prime(V \boldsymbol{s_i}) \cdot V \cdot g^\prime(U \boldsymbol{x_i} + W \boldsymbol{s_{i-1}}) \cdot  \left[ \boldsymbol{x_{i}} + W \cdot g^\prime (U \boldsymbol{x_{i-1}} + W \boldsymbol{x_{i-2}}) \cdot \frac{\partial (U \boldsymbol{x_{i-1}} + W \boldsymbol{s_{i-2}})} {\partial U} \right] \\
&= \frac{\partial J}{\partial \hat{y}_i} \cdot O^\prime(V \boldsymbol{s_i}) \cdot V \cdot g^\prime(U \boldsymbol{x_i} + W \boldsymbol{s_{i-1}}) \cdot  \left[ \boldsymbol{x_{i}} + W \cdot g^\prime (U \boldsymbol{x_{i-1}} + W \boldsymbol{s_{i-2}}) \cdot \left[ \boldsymbol{x_{i-1}} + W \cdot \frac{\partial (\boldsymbol{s_{i-2}})} {\partial U} \right] \right] \\
& = \ldots
\end{align*}}
\endgroup

Backpropagation through time carries two major drawbacks. The first one is that due to its recursive nature it is computationally very expensive.  However the second one,  and most important, is the one that ti makes it also a bit useless.  Notice that the gradients of both $W$ and $U$ include the weights $W$ themselves multiple times.  If we expand the formulas and we do the calculation we will find out that the formulas actually contain a lot of increasing powers of $W$,  i.e: $W, W^2, W^3, \ldots, W^n$.  This has as a result the following:
\bit
\item If $W < 1$ then $W^i \to 0, \textit{as } i \to n$.  In other words we are dealing with the problem of ``vanishing gradients" which has as a result the terms due to further back time steps to have smaller and smaller gradients hence to contribute less and less.  This adds  bias parameter to the model since it captures only short term dependencies.
\item If $W > 1$ then $W^i \to \infty,  \textit{as } i \to n$.  In other words we are dealing with the problem of ``exploding gradients" which has as a result for the backpropagation through time to have bigger and bigger steps as time passes having as a result non-convergence to the optimal point.
\eit

There are a couple of ways to deal with these problems. 
\bit
\item \textbf{ReLU Activation Function}: By using ReLU as activation function we force the gradients to be 1 since the derivative of ReLU is 1 when $x>0$.
\item \textbf{Gradient Clipping}: With gradient clipping, a predetermined gradient threshold is introduced,  and then gradients norms that exceed this threshold are scaled down to match the norm.  This prevents any gradient to have norm greater than the threshold and thus the gradients are clipped.  There is an introduced bias in the resulting values from the gradient,  but gradient clipping can keep things stable. 
\item \textbf{Truncated Backpropagation Through Time:} In truncated backpropagation through time we truncate the sum after a predetermined number of steps $\tau$.  This leads to an approximation of the true gradient,  simply by ignoring higher terms of $W$.  In practice this works quite well however it focuses primarily on short-term influence rather than long-term consequences. 
\item \textbf{Parameter Initialization}: It is shown that by initializing weights to 1 and biases to 0 helps from preventing the parameters to shrink to 0.
\eit

However the best, and most used,  way of dealing with this problem is to build more complex RNNs that are able to control what information is passed through.  By far,  the most heavily used model is one called ``Long Short Term Memory" or more simply LSTM.

\subsection{Long Short Term Memory (LSTM)}

