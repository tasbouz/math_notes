\section{Word Embedding}

In natural language processing (NLP),  word embedding is a term used for the vectorial representation of words for text analysis,  typically in the form of a real valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning.  Word embeddings can be obtained using a set of language modelling and feature learning techniques where words or phrases from the vocabulary are mapped to vectors of real numbers.  Conceptually it involves a mathematical embedding from a space with many dimensions per word to a continuous vector space with a much lower dimension. \v

Let us start with a very simple motivation for why we are interested in a vectorial representations of words. Suppose we are given an input stream of words like a sentence or a document and we are interested in learning some function of it e.g: $\hat{y} = \textit{sentiments(words)}$.  Say,  we employ a machine learning algorithm (some mathematical model) for learning such a function $\hat{y} = f(x)$. We first need a way of converting the input stream (or each word in the stream) to a vector.  Let's start with two important definitions.\v

\bd[Corpus]
A \textbf{corpus} is a language resource consisting of a large and structured set of texts. 
\ed

\bd[Vocabulary]
Given a corpus, a \textbf{vocabulary} $V$ is the set of all unique words in the corpus.
\ed

There are two big classes of methods of word embedding: the ``count based models" which they use the co-occurrence counts of words, and the ``prediction based models" which directly learn word representations.  The first class of count based models were the one used before the discovery of deep learning while prediction based models are the latest advancements in the area of word embedding that make use of deep learning. We will begin by briefly introducing the count based models, since they will be useful for later, and then we switch to the main part of prediction based models. 

\subsection{One-Hot Representation}

Word embedding is actually the attempt of a vector representation for every word in a vocabulary $V$.  The simplest way to do so is the so called ``one-hot'' representation.

\bd[One-Hot Representation]
One-hot representation is a group of bits among which the legal combinations of values are only those with a single high 1 bit and all the others low 0.  A similar implementation in which all bits are 1 except one 0 is sometimes called `one-cold''. 
\ed

The following is an example of a one-hot representation of the vocabulary $V = [$ ``cat'',  ``dog'', ``truck'' $]$:

\begin{figure}[H]
\includegraphics[scale=0.4]{images/onehot.png}
\centering
\end{figure}

While one-hot representation is very simple and intuitive, it carries a lot of problems. First of all,  usually vocabularies tend to be very large hence the vector space turns enormous which is computationally very inefficient.  Second,  the representation is very sparse, meaning that the the vast majority of the entries are zeros. Third, and most important,  the representation does not capture any notion of similarity among the words. In our example,  ideally,  we would want the representations of cat and dog to be closer to each other
than the representations of cat and truck.  However, with one-hot representation, the Euclidean distance between any
two words in the vocabulary is simply $\sqrt{2}$.  Similarly the cosine similarity between any two words in the vocabulary is
$0$.

\subsection{Distributed Representation}

We will now introduce the distributional similarity based representation. The idea behind distributed representation is expressed in the phrase ``you shall know a word by the company it keeps''. 

\bd[Co-Occurrence Matrix]
Given a vocabulary $V$ with $n$ terms, the \textbf{co-occurrence matrix} is an $(n \times n)$ matrix which captures the number of times a term appears in the context of another term,  where context is defined as a window of $k$ words around the terms.  Each row (or column) of the co-occurrence matrix gives a vectorial representation of the corresponding word.
\ed

As an example,  let us build a co-occurrence matrix for a toy corpus with $k = 2$.  The corpus consist of the following sentences:
\bit
\item Human machine interface for computer applications.
\item User opinion of computer system response
time.
\item User interface management system.
\item System engineering for improved response
time.
\item System engineers optimize for human experience.
\eit

The vocabulary looks like this: $V = [$ ``Human",  ``Machine",  ``Interface", ``For",  ...  $]$.  Finally,  the co-occurrence matrix looks like this:

\begin{figure}[H]
\includegraphics[scale=0.65]{images/cooccurance.png}
\centering
\end{figure}

Of course,  as with one-hot,  also this representation carries its own problems.  First of all, stop words like (``a",  ``the", `` for",  ...) are very frequent hence these counts will be very high.  This however,  is easily solvable since we can either ignore very frequent words or simply set a threshold $t$ (for example $t=100$) and whenever the count of a word gets more that $t$ we simply stop counting further and we use the threshold $t$ as the value,  ignoring all the other occurrences.  \v

The most important problems of distributed representation though are that,  as in hot-one representation,  it is very high dimensional, it is very sparse and it grows with the size of the vocabulary.  All of these problems were solved by using singular value decomposition as a dimensionality reduction technique.  However,  we will not see that in more detail since it is not related to deep learning and it gets out of topic. \v

For many years the singular value decomposition of distributed representation was the way to go for NLP and word embedding problems.  However,  in 2006 and after,  after the discovery of deep learning,  we switched gears from the whole idea of counting word occurrences and co-occurrence matrices of count based models to direct,  prediction based models that uses statistics to predict outcomes.  In what follows we will introduce the most heavily used group of techniques of prediction based models called ``Word2vec''.

\subsection{Word2vec}

Word2vec is a technique for natural language processing that was created,  patented,  and published in 2013 by a team of researchers led by Tomas Mikolov at Google over two papers.  The word2vec algorithm uses a neural network model to learn word associations from a large corpus.  Once trained,  such a model can detect synonymous words or suggest additional words for a partial sentence.  As the name implies,  word2vec represents each distinct word with a particular list of numbers called a vector. The vectors are chosen carefully such that a simple mathematical function (the cosine similarity between the vectors) indicates the level of semantic similarity between the words represented by those vectors.  Embedding vectors created using the word2vec algorithm have some advantages compared to earlier algorithms that we described previously.\v

Word2vec is a group of related models that are used to produce word embeddings. These models are shallow,  two-layer neural networks that are trained to reconstruct linguistic contexts of words.  Word2vec takes as its input a large corpus of text and produces a vector space,  typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space.  Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located close to one another in the space. \v

As we mentioned, word2vec can utilize either of two model architectures to produce a distributed representation of words: ``continuous bag-of-words'' (CBOW) or ``continuous skip-gram''.   In the continuous bag-of-words architecture, the model predicts the current word from a window of surrounding context words. The order of context words does not influence prediction (bag-of-words assumption).  In the continuous skip-gram architecture,  the model uses the current word to predict the surrounding window of context words. The skip-gram architecture weighs nearby context words more heavily than more distant context words. According to the authors' note CBOW is faster while skip-gram does a better job for infrequent words. In what follows we will introduce both of them since they are the standard way of dealing with word embeddings today.

\subsubsection{Continuous Bag-Of-Words (CBOW)}

The Continuous Bag-Of-Words (CBOW) model tries to understand the context of the words and takes this as input.  It then tries to predict words that are contextually accurate.  In other words it tries to predict the current target word based on the source context words (surrounding words).  \v

Let's see an example.  Consider the simple sentence ``...  the man sat on a chair",  this can be seen as pairs of (context,  word) where if we consider a context window of size 2,  we have examples like ([the],  man), ([the, brown], quick), ([the, dog], lazy) and so on.  Thus the model tries to predict the target word based on the context words.  \v

We will develop the CBOW model for this problem by using a feedforward neural network.  The input to the network will be a one-hot representation of the
context word and the output, will be a probability distribution over all possible words in the vocabulary (multi-class classification problem). 























\subsubsection{Continuous Skip-Gram}