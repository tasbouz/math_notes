In this chapter we will introduce another way of inference called \say{parametric inference}. In parametric inference the goal is given a sample of $n$ realizations of i.i.d r.v's to learn the underlying distribution $P$ of $X$. Let's begin with some basic definitions.

\section{Basic Definitions}

\begin{definition}[Statistical Model]
Let the observed outcome of a random experiment (definition of random experiment at (\ref{def:random_experiment})) be a sample $\{ X_{1}, X_{2}, \ldots, X_{n} \}$ of $n$ i.i.d r.v's in some measurable space $E \subseteq R$, and $P$ their common distribution. A \textbf{statistical model} associated to that statistical experiment is the tuple $(E, (P_{\theta})_{\theta \in \Theta})$ where:

\begin{itemize}
    \item $E$: Sample space where $X$ lives.
    \item $P_{\theta}$: Family of probability measures.
    \item $\Theta$: Parameter set (usually $\Theta = R^d$)
\end{itemize}
\end{definition}

Let's see some examples of statistical models.

\begin{itemize}
    \item Bernoulli Statistical Model: $( \{ 0,1 \}, \:\:\: (Bern(p))_{ {p} \in [0,1]})$
    
    \vspace{5pt}
    
    \item Exponential Statistical Model: $( (0,\infty), \:\:\: (Expo(\lambda))_{ \lambda \in [0,\infty)}$
    
    \vspace{5pt}

    \item Poisson Statistical Model: $( N, \:\:\: (Pois(\lambda))_{ \lambda \in [0,\infty)}$

    \vspace{5pt}

    \item Gaussian Statistical Model: $( R, \:\:\: (N(\mu, \sigma^2))_{\mu \in R, \:\:\: \sigma^2 \in (0,\infty) }$
    
    \vspace{5pt}
    
    \item Uniform Statistical Model: $( [0, \infty), \:\:\: (Unif(a,b))_{a \in [0,\infty), \:\:\: n \in [0,\infty)})$
\end{itemize}

\vspace{5pt}

Intuitively, given a set of observed outcomes $\{ X_{1}, X_{2}, \ldots, X_{n} \}$ we will assume that they follow some broad family of probability measures $P_{\theta}$ parametrized by some parameter $\theta$ (e.g: a Bernoulli distribution Bern(p) where $\theta = p$). Our goal is based on the given sample to specify this parameter and subsequently the underlying distribution.

\begin{definition}[True Parameter]
We always make the assumption that $\exists \: \theta^{*} \in \Theta : X \sim P_{\theta}$. We call this specific $\theta^{*}$ the \textbf{true parameter}.
\end{definition}

\begin{definition}[Estimator]
An \textbf{estimator} $\hat{\theta}_{n}$ is a function that maps the sample space to a set of sample estimates.

\begin{equation}
    \hat{\theta}_{n} = \hat{\theta}_{n}(X_{1}, X_{2}, \ldots, X_{n})
\end{equation}
\end{definition}

Notice that since $\{ X_{1}, X_{2}, \ldots, X_{n} \}$ are r.v's subsequently $\hat{\theta}_{n}$ is also a r.v since it is a function of r.v's. The true parameter on the other hand is a deterministic real number. \\

From the law of large numbers, we get that for the estimator holds:
\begin{equation}
    \lim_{n\to\infty} \hat{\theta}_{n} = \theta
\end{equation}

\vspace{5pt}

Given the true parameter and the estimator, we can define some measures of error as follows.

\begin{definition}[Error]
Given a true parameter $\theta$ and an estimator $\hat{\theta}_{n}$, we define the \textbf{error} $e$ of the estimator $\hat{\theta}_{n}$ as:
\begin{equation}
    e = \hat{\theta}_{n} - \theta
\end{equation}
\end{definition}

\begin{definition}[Mean Squared Error]
Given a true parameter $\theta$ and an estimator $\hat{\theta}_{n}$, we define the \textbf{mean squared error} (MSE) of the estimator $\hat{\theta}_{n}$ as:
\begin{equation}
    MSE = E[(\hat{\theta}_{n} - \theta)^2]
\end{equation}
\end{definition}

\begin{definition}[Efficient Estimator]
An estimator $\hat{\theta}_{n}$ is called \textbf{efficient} if MSE is sufficient small.
\end{definition}

\begin{definition}[Consistent Estimator]
An estimator $\hat{\theta}_{n}$ is called \textbf{consistent} if:
\begin{equation*}
    \lim_{n \to \infty} MSE = 0
\end{equation*}
\end{definition}

\begin{definition}[Sampling Deviation]
Given a true parameter $\theta$ and an estimator $\hat{\theta}_{n}$, we define the \textbf{sampling deviation} $d$ of the estimator $\hat{\theta}_{n}$ as:
\begin{equation}
    d = \hat{\theta}_{n} - E[\hat{\theta}_{n}]
\end{equation}
\end{definition}

\begin{definition}[Bias]
Given a true parameter $\theta$ and an estimator $\hat{\theta}_{n}$, we define the \textbf{bias} $B$ of the estimator $\hat{\theta}_{n}$ as:
\begin{align}
    B = E[\hat{\theta}_{n} - \theta] = E[\hat{\theta}_{n}] - E[\theta] = E[\hat{\theta}_{n}] - \theta
\end{align}
\end{definition}

\begin{definition}[Unbiased Estimator]
An estimator $\hat{\theta}_{n}$ is called \textbf{unbiased} if $B=0$.
\end{definition}

\begin{definition}[Variance]
Given a true parameter $\theta$ and an estimator $\hat{\theta}_{n}$, we define the \textbf{variance} Var of the estimator $\hat{\theta}_{n}$ as:
\begin{equation}
    Var = E[(\hat{\theta}_{n} - E[\hat{\theta}_{n}])^2]
\end{equation}
\end{definition}

\vspace{5pt}

By manipulating MSE we can show:
\begin{align*}
    MSE &= E[(\hat{\theta}_{n} - \theta)^2] \\
        &= E[\hat{\theta}_{n}^2 - 2\hat{\theta}_{n}\theta + \theta^2] \\
        &= E[\hat{\theta}_{n}^2] - E[2 \hat{\theta}_{n} \theta] + E[\theta^2] \\
        &= E[\hat{\theta}_{n}^2] - 2 \theta E[\hat{\theta}_{n}] + \theta^2 \\
        &= E[\hat{\theta}_{n}^2] - 2 \theta E[\hat{\theta}_{n}] + \theta^2 + E^2[\hat{\theta}_{n}] - E^2[\hat{\theta}_{n}]  \\
        &= (E[\hat{\theta}_{n}^2] - E^2[\hat{\theta}_{n}]) + (E^2[\hat{\theta}_{n}] - 2 \theta E[\hat{\theta}_{n}] + \theta^2) \\
        &= (E[\hat{\theta}_{n}^2] - E^2[\hat{\theta}_{n}]) + (E[\hat{\theta}_{n}] - \theta)^2 \\
        &= Var + B^2
\end{align*}

Hence we showed that the MSE of an estimator can actually be split into the variance and the bias of the estimator. 

\section{Maximum Likelihood}

As we explained in the previous section our goal in parametric inference is given a statistical model $(E, P_{\theta})$ associated with a sample of i.i.d r.v's $\{ X_{1}, X_{2}, \ldots, X_{n} \}$, by making the assumption that there always exists a true parameter $\theta^{*}$ such that $X \sim P_{\theta^{*}}$, to find this true parameter.\\ 

Our initial step is to define an estimator $\hat{\theta}_{n}$, that subsequently defines a probability distribution $P_{\hat{\theta}_{n}}$. Hence now we have two quantities: the probability distribution we actually want to find $P_{\theta^{*}}$ and the probability distribution that we begin with $P_{\hat{\theta}_{n}}$. Since the difference of these two is what we want to minimize, it makes sense to define the absolute distance between them as follows:

\begin{definition}[Total Variation Distance]
The \textbf{total variation distance} TV of two probability distributions $P_{\theta}$ and $P_{\theta^{\prime}}$ is defined as the largest possible difference between the probabilities that the two probability distributions can assign to the same event.

\begin{equation}
    TV(P_{\theta}, P_{\theta}^{\prime}) = \max_{A \subseteq E} |P_{\theta}(A) - P_{\theta}^{\prime}(A)|
\end{equation}
\end{definition}

\vspace{5pt}

The total variation distance satisfies the following properties:

\begin{itemize}
    \item $TV(P_{\theta}, P_{\theta}^{\prime}) = TV(P_{\theta}^{\prime}, P_{\theta})$
    \item $TV(P_{\theta}, P_{\theta}^{\prime}) \geq 0$
    \item $TV(P_{\theta}, P_{\theta}^{\prime}) =0 \Rightarrow P_{\theta} = P_{\theta}^{\prime}$
    \item $TV(P_{\theta}, P_{\theta}^{\prime}) \leq TV(P_{\theta}, P_{\theta}^{\prime\prime}) + TV(P_{\theta^{\prime\prime}}, P_{\theta}^{\prime}) $
\end{itemize}

These properties imply that total variation distance is indeed a distance measure between probability distributions (hence the name). \\

Due to the \say{max} term that appears in  total variation distance it's very hard to manipulate it. Fortunately, there is an alternative way of describing the same concept. As we see in the figure below, since total variation distance is just the absolute difference of two probabilities it can also be expressed as the two times the area under the curve that it is not common in the two distributions. (The white area in the graph)

\vspace{10pt}

\begin{figure}[H]
\includegraphics[scale=0.75]{images/totalvariationdistance.png}
\centering
\caption {Total Variation Distance Between $P_{\theta}$ and $P_{\theta}^{\prime}$}
\end{figure}

\vspace{10pt}

Moreover, by defining the area $A^*$ as the area in which $P_{\theta} \geq P_{\theta}^{\prime}$, we can get rid of the max in total variation distance as:
{\setlength{\jot}{15pt}
\begin{align*}
    TV(P_{\theta}, P_{\theta}^{\prime}) & = \max_{A \subseteq E} |P_{\theta}(A) - P_{\theta}^{\prime}(A)| \\
                                        &= |P_{\theta}(A^*) - P_{\theta}^{\prime}(A^*)| && \text{($A^* = {x: P_{\theta} \geq P_{\theta}^{\prime}}$)} \\
                                        &= P_{\theta}(A^*) - P_{\theta}^{\prime}(A^*) && \text{($P_{\theta} \geq P_{\theta}^{\prime}$)} \\
                                        &= \int_{A^*} (p_{\theta}(x) - p_{\theta}^{\prime}(x)) dx && \text{(definition of a PDF)} \\
                                        &= \frac{1}{2} \int_{A^*} (p_{\theta}(x) - p_{\theta}^{\prime}(x)) dx + \frac{1}{2} \int_{(A^*)^C} (p_{\theta}^{\prime}(x) - p_{\theta}(x)) dx && \text{($(A^*)^C = {x: P_{\theta}^{\prime}  \geq P_{\theta}}$)}  \\
                                        &= \frac{1}{2} \int_{E} |p_{\theta}(x) - p_{\theta}^{\prime}(x)| dx && \text{($E = A^* \cup (A^*)^C$)}
\end{align*}}

Hence we showed that:

\begin{equation} \label{eq:tv}
    TV(P_{\theta}, P_{\theta}^{\prime}) = \frac{1}{2} \int_{E} |p_{\theta}(x) - p_{\theta}^{\prime}(x)| dx
\end{equation}

\vspace{5pt}

and similarly for a discrete distribution:

\begin{equation}
    TV(P_{\theta}, P_{\theta}^{\prime}) = \frac{1}{2} \sum_{x \in E} |p_{\theta}(x) - p_{\theta}^{\prime}(x)|
\end{equation}

\vspace{5pt}

Coming back to our case if we set $\theta = \hat{\theta}$ to be our estimator and $\theta^{\prime} = \theta^{*}$ to be our true parameter, then by using (\ref{eq:tv}) we have an equation for the total variation distance that includes something that we can work with (i.e PDF's). And since, as we argued, total variation distance is just the distance between the two probabilities, our goal is to minimize it as much as possible. That way the distance between our estimator probability and true probability will be as low as it can be, hence very the estimation will be very close to the real distribution. \\

The problem with (\ref{eq:tv}) is that it carries an absolute value which makes it impossible to minimize it. For this reason we are gonna introduce another quantity on top of total variation distance called \say{Kullback - Leibler divergence}.

\begin{definition}[Kullback - Leibler Divergence]
The \textbf{Kullback - Leibler divergence} KL of two probability distributions $P_{\theta}$ and $P_{\theta^{\prime}}$ is defined in term of their PDF's as follows:

\begin{equation} \label{eq:KL}
    KL(P_{\theta}, P_{\theta}^{\prime}) = \int_{E} p_{\theta}(x) \ln \frac{p_{\theta}}{p_{\theta}^{\prime}(x)} dx
\end{equation}
\end{definition}

\vspace{5pt}

The Kullback - Leibler divergence satisfies the following properties:

\begin{itemize} 
    \item $KL(P_{\theta}, P_{\theta}^{\prime}) \neq KL(P_{\theta}^{\prime}, P_{\theta})$
    \item $KL(P_{\theta}, P_{\theta}^{\prime}) \geq 0$
    \item $KL(P_{\theta}, P_{\theta}^{\prime}) =0 \Rightarrow P_{\theta} = P_{\theta}^{\prime}$
    \item $KL(P_{\theta}, P_{\theta}^{\prime}) \nleq KL(P_{\theta}, P_{\theta}^{\prime\prime}) + KL(P_{\theta^{\prime\prime}}, P_{\theta}^{\prime})$
\end{itemize}

These properties imply that Kullback - Leibler divergence is not a distance measure between probability distributions (hence the name divergence). \\

By manipulating (\ref{eq:KL}) we get:
\begin{align} \label{eq:KLnew}
    KL(P_{\theta}, P_{\theta}^{\prime}) = \int_{E} p_{\theta}(x) \ln \frac{p_{\theta}}{p_{\theta}^{\prime}(x)} dx = E_{\theta} [\ln \frac{p_{\theta}}{p_{\theta}^{\prime}}] = E_{\theta} [\ln p_{\theta} - \ln{p_{\theta}^{\prime}}] = E_{\theta} [\ln p_{\theta}] - E_{\theta}[\ln{p_{\theta}^{\prime}}]
\end{align}

Coming back to our case if we set $\theta = \theta^{*}$ to be our estimator and $\theta^{\prime} = \hat{\theta}$ to be our true parameter, then by using (\ref{eq:KLnew}) we have:
\begin{align*}
    KL(P_{\theta^{*}}, P_{\hat{\theta}}) = E_{\theta^{*}} [\ln p_{\theta^{*}}] - E_{\theta^{*}}[\ln {p_{\hat{\theta}}}] = c - E_{\theta^{*}}[\ln {p_{\hat{\theta}}}]
\end{align*}

As we said, Kullback - Leibler divergence is not a distance measure however it still is a good measure to minimize in order to find a good estimation for the true probability distribution. Hence the estimator is simply the argument that minimizes Kullback - Leibler divergence:
\begin{align*}
\hat{\theta} = \argmin_{\theta} \Big( KL(P_{\theta^{*}}, P_{\theta}) \Big) = \argmin_{\theta} \Big( c - E_{\theta^{*}}[\ln {p_{\theta}}] \Big) =\argmin_{\theta} ( c ) - \argmin_{\theta} \Big( E_{\theta^{*}}[\ln {p_{\theta}}] \Big)
\end{align*}

The term $\argmin(c)$ is just a constant that does not depend on $\theta$. Same $\theta$ that minimizes $f(\theta)$ minimizes also $c - f(\theta)$. So we can just drop it from the equation and get:
\begin{align*}
\hat{\theta} =  - \argmin_{\theta} \Big( E_{\theta^{*}}[\ln {p_{\theta}}] \Big)
\end{align*}

As we discussed previously we can approximate the expected value with sample mean $E_{\theta^{*}} \to \frac{1}{n} \sum_{i}$:
\begin{align*}
\hat{\theta} &= - \argmin_{\theta} \Big( \frac{1}{n} \sum_{i=1}^{n} \ln {p_{\theta}}(x_{i}) \Big) \\
                 &= - \argmin_{\theta} \Big( \sum_{i=1}^{n} \ln {p_{\theta}}(x_{i}) \Big) \\
                 &= \argmax_{\theta} \Big( \sum_{i=1}^{n} \ln {p_{\theta}}(x_{i}) \Big) \\
                 &= \argmax_{\theta} \Big( \ln \prod_{i=1}^{n} {p_{\theta}}(x_{i}) \Big) \\
                 &= \argmax_{\theta} \Big( \prod_{i=1}^{n} {p_{\theta}}(x_{i}) \Big)
\end{align*}

Based on this last two equation we define the concept of \say{likelihood} as follows:

\begin{definition}[Likelihood]
Let X be a random variable following a probability distribution with probability density function $f_{\theta}(x)$ depending on a parameter $\theta$ . Then the \textbf{likelihood} $\mathcal {L} (\theta \mid x)$ is formed from the joint probability of a sample of data of $X$:

\begin{equation}
    \mathcal {L} (\theta \mid x) =  \prod_{i=1}^{n} {f_{\theta}}(x_{i})
\end{equation}
\end{definition}

More often than not, it is more handy to use the logarithm of the likelihood which we define as the \say{log-likelihood}.

\begin{definition}[Log-likelihood]
The \textbf{log-likelihood} $l(\theta \mid x)$ is simply the logarithm of the likelihood:

\begin{equation}
    l(\theta \mid x) =  \ln \mathcal {L} (\theta \mid x)
\end{equation}
\end{definition}

So we can find the estimator $\hat{\theta}$ by maximizing the likelihood or the log-likelihood, i.e:
\begin{equation}
    \frac{d \mathcal {L}}{d\theta} \Big|_{\theta=0} = 0 \qquad \text{or} \qquad \frac{dl}{d\theta} \Big|_{\theta=0}
\end{equation}

The solution of either of these equations gives back the best estimator. This process is called \say{the principle of maximum likelihood} or simply \say{the maximum likelihood method}.

\section{Application: Maximum Likelihood In Bernoulli Distribution}

Let us have a collection of i.i.d r.v's $\{ X_{1}, X_{2}, \ldots, X_{n} \}$ following a Bernoulli distribution Bern(p) (i.e they are described by probability distribution (\ref{eq:bernoulli_pmf})).. Our goal is based on the sample $\{ X_{1}, X_{2}, \ldots, X_{n} \}$ to estimate the value of the parameter $p$.\\

We start by forming the likelihood:
\begin{align*} \mathcal {L} =  \prod_{i=1}^{n} P(x_{i})  = \prod_{i=1}^{n} p^{x_{i}} (1-p)^{1-x_{i}}
\end{align*}

Subsequently for the log-likelihood:
\begin{align*}
    l = \ln \mathcal {L} = \ln \prod_{i=1}^{n} p^{x_{i}} (1-p)^{1-x_{i}}                                
\end{align*}

Now we can manipulate the log-likelihood to obtain a more handy expression:
{\setlength{\jot}{10pt}
\begin{align*}
l &= \ln \prod_{i=1}^{n} p^{x_{i}} (1-p)^{1-x_{i}} \\
  &= \sum_{i=1}^{n} \Big[ \ln ( p^{x_{i}} (1-p)^{1-x_{i}}) \Big] \\
  &= \sum_{i=1}^{n} \Big[ \ln p^{x_{i}} + \ln (1-p)^{1-x_{i}} \Big] \\
  &= \sum_{i=1}^{n} \Big[ x_{i} \cdot \ln p + (1-x_{i}) \cdot \ln (1-p) \Big] \\
  &= \sum_{i=1}^{n} \Big[ x_{i} \cdot \ln p  \Big] + \sum_{i=1}^{n} \Big[ (1-x_{i}) \cdot \ln (1-p) \Big] \\
  &= \ln p \cdot \sum_{i=1}^{n} \Big[ x_{i} \Big] + \ln (1-p) \cdot \sum_{i=1}^{n} \Big[ (1-x_{i}) \Big] \\
    &= n  \cdot \ln p \cdot \sum_{i=1}^{n} \Big[ \frac{1}{n} x_{i} \Big] + n \cdot \ln (1-p) \cdot \sum_{i=1}^{n} \Big[ \frac{1}{n} (1-x_{i}) \Big] \\
    &= n \cdot \ln p \cdot \bar{x} + n  \cdot \ln (1-p) \cdot (1 - \bar{x}) \\
    &= n (\bar{x} \ln p  +(1 - \bar{x}) \ln (1-p))
\end{align*}}

For the derivative of the log-likelihood we obtain:
\begin{align*}
    \frac{dl}{dp} =  \frac{d}{dp} \Big( n (\bar{x} \ln p  + (1 - \bar{x}) \ln (1-p)) \Big) = n \Big( \frac{\bar{x}}{p} - \frac{1 - \bar{x}}{1-p} \Big)  = \ldots = n(\bar{x} - p)
\end{align*}

Finally, from principle of maximum likelihood, we can obtain the best estimator by setting the derivative to zero:
\begin{align*}
    \frac{dl}{dp} = 0 \Rightarrow p = \bar{x}
\end{align*}

Hence, we proved that the estimator that maximizes the likelihood for a Bernoulli distribution is actually the average of the sample. In a similar way we can show the same for all the distributions we have introduced.\\

Now let's compute some of the characteristics of this estimator.\\

For the error of the estimator:
\begin{align*}
e = \hat{\theta}_{n} - \theta = \bar{x} - p
\end{align*}
    
For the sampling deviation:
{\setlength{\jot}{10pt}
\begin{align*}
d &= \hat{\theta}_{n} - E[\hat{\theta}_{n}] \\
   &= \bar{x} - E[\bar{x}] \\
   &= \bar{x} - E \Big[ \frac{1}{n} \sum_{i=1}^{n} x_{i} \Big] \\
   &= \bar{x} - \frac{1}{n} \sum_{i=1}^{n} E[x_{i}] \\
   &= \bar{x} - \frac{1}{n} \sum_{i=1}^{n} p \\
   &= \bar{x} - \frac{1}{n} n p  \\
   &= \bar{x} - p
\end{align*}}
    
For the bias: 
\begin{align*}
E[\hat{\theta}_{n}] - \theta = E[\bar{x}] - p = p - p = 0
\end{align*}

Hence the estimator $\bar{x}$ is unbiased.

For the variance:
{\setlength{\jot}{10pt}
\begin{align*}
        Var(\theta) &= Var(\bar{x}) \\
        					&= Var(\frac{1}{n} \sum_{i=1}^{n} x_{i}) \\
        					&= \frac{1}{n^2} Var(\sum_{i=1}^{n} x_{i})\\
        					&= \frac{1}{n^2} \sum_{i=1}^{n} Var(x_{i}) \\
        					&= \frac{1}{n^2} \sum_{i=1}^{n} p(1-p) \\
        					&= \frac{n p(1-p)}{n^2}  \\
        					& = \frac{p(1-p)}{n}
\end{align*}}

For the mean Squared Error:
\begin{align*}
        MSE = Var + B^2 = \frac{p(1-p)}{n}+ 0^2 = \frac{p(1-p)}{n}
\end{align*}
    
Finally observe that since $MSE \propto \frac{1}{n}$ we have that as $n \to \infty$, $MSE \to 0$,  so the estimator $\bar{x}$ is consistent.