Reinforcement learning lies between supervised and unsupervised learning.  Namely while there is no supervision,  there is a reward for every step/ decision acting like a feedback that usually comes delayed after few steps.  This makes time really important since we are dealing with sequential and not i.i.d data and most importantly the actions of the agent affect the subsequent data it receives. In other words we deal with dynamical environments with high dependencies. \v

The idea that we learn by interacting with our environment is probably the first to occur to us when we think about the nature of learning.  Interactions produce a wealth of information about cause and effect,  about the consequences of actions,  and about what to do in order to achieve goals.  Throughout our lives, such interactions are undoubtedly a major source of knowledge about our environment and ourselves.  Learning from interaction is a foundational idea underlying nearly all theories of learning and intelligence.  \v

Reinforcement learning is simultaneously a problem,  a class of solution methods that work well on the class of problems,  and the field that studies these problems and their solution methods.  Reinforcement learning problems involve learning what to do, how to map situations to actions, so as to maximize a numerical reward signal.  In an essential way they are closed-loop problems because the learning system's actions influence its later inputs.  Moreover, the learner is not told which actions to take but instead must discover which actions yield the most reward by trying them out.  In the most interesting and challenging cases,  actions may affect not only the immediate reward but also the next situation and,  through that,  all subsequent rewards. \v

An agent must be able to sense the state of the environment to some extent and must be able to take actions that affect the state. The agent also must have a goal relating to the state of the environment. The formulation is intended to include just these three aspects, i.e sensation, action, and goal, in their simplest possible forms without trivializing any of them. Any method that is well suited to solving this kind of problem we consider to be a reinforcement learning method.  \v

One of the challenges that arise in reinforcement learning,  and not in other kinds of learning,  is the trade-off between exploration and exploitation. To obtain a lot of reward, a reinforcement learning agent must prefer actions that it has tried in the past and found to be effective in producing reward.  But to discover such actions,  it has to try actions that it has not selected before.  The agent has to exploit what it already knows in order to obtain reward,  but it also has to explore in order to make better action selections in the future. The dilemma is that neither exploration nor exploitation can be pursued exclusively without failing at the task. The agent must try a variety of actions and progressively favour those that appear to be best.   \v

Another key feature of reinforcement learning is that it explicitly considers the whole problem of a goal-directed agent interacting with an uncertain environment.  This is in contrast with many approaches that consider sub-problems without addressing how they might fit into a larger picture.  Reinforcement learning takes the opposite tack, starting with a complete,  interactive,  goal-seeking agent.  All reinforcement learning agents have explicit goals,  can sense aspects of their environments,  and can choose actions to influence their environments.  Moreover,  it is usually assumed from the beginning that the agent has to operate despite significant uncertainty about the environment it faces.  When reinforcement learning involves planning, it has to address the interplay between planning and real-time action selection, as well as the question of how environment models are acquired and improved. 

\section{Introduction \& Basic Terminology}
\bd[Reinforcement Learning]
\textbf{Reinforcement learning} is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward.
\ed

In simple words reinforcement learning tries to optimize a sequence of decisions.  In what follows we will define the basic elements of reinforcement learning problems.  There are a lot of definitions but they cover the entirety of all reinforcement learning problems. 

\bd[Agent]
An \textbf{agent} is an autonomous entity which acts,  directing its activity towards achieving goals upon an environment using observations through sensors and consequent actuators. 
\ed

An agent should,  among others,  accommodate new problem solving rules incrementally and adapt online and in real time.  It should be also able to analyse itself in terms of behaviour,  error and success. When it comes to learning it should learn quickly from large amounts of data and improve through interaction with the environment.  Finally it should
have a memory-based exemplar storage and retrieval capacities and parameters to represent short and long term memory.

\bd[State]
A \textbf{state} $S$ is a representation of the current situation of an agent, or an environment.
\ed

In simple words a state is all the information an agent uses in order to determines what happens next.  We distinguish between the ``agent state" and ``environment state".

\bd[Agent State]
\textbf{Agent state} is the agent's internal representation.
\ed

\bd[Environment State]
\textbf{Environment  state} is the environment's private (not known to agent) representation.
\ed

\bd[Action]
An \textbf{action} $A$ is the choice of action by an agent given a state. 
\ed

\bd[Reward]
A \textbf{reward} $R$ is an abstract concept that describes a scalar feedback signal an agent receives from a given state in an environment,  given its action on the state. 
\ed

A reward can be positive or negative.  When the reward is positive,  it is corresponding to our normal meaning of reward.  When the reward is negative,  it is corresponding to what we usually call ``punishment".

\bd[Reward Hypothesis]
\textbf{Reward Hypothesis} states that all goals of an agent can be described by the maximization of an expected cumulative reward.
\ed

Hence the agent's goal is to select actions in order to maximize the future total expected cumulative reward, given that each action might have long term consequences hence a reward might be delayed. This makes reinforcement learning problems quite complicated since in some cases it might be better to sacrifice an immediate reward to gain more long term reward.  \v

We have used the term ``environment" many times without having properly define it.  At this point we have all the ingredients needed in order to give a definition.

\bd[Environment]
An \textbf{environment} is the entity that receives the action from an agent and emits a state and a reward.
\ed

An environment can be either ``fully observable" or ``partially observable".

\bd[Fully Observable Environment]
A \textbf{fully observable environment} is the case where the agent directly observes the environment state.  In this case the agent's state and the environment's state coincide.
\ed

\bd[Partially Observable Environment]
A \textbf{partially observable environment}  is the case where the agent indirectly observes the environment state.  In this case the agent's state is different than the environment's state and the agent must construct its own state representation.
\ed

In a fully observable environment the agent receives as a state the whole environment state and it uses it as its own state to take a decision.  Fully observable environments is the main framework used in reinforcement learning since they constitute the vast majority of reinforcement learning problems.  \v

Let's summarize the definitions we have provided so far.  An agent within an environment observes an initial state $S_1$ it is in, and it takes an action $A_1$. The action $A_1$ is received by the environment which, by its turn,  it emits a reward $R_1$ and a new state $S_2$.  The reward $R_1$ informs the agent how good the action $A_1$ was,  and the state $S_2$ makes the agent to take a new action $A_2$,  and so on.

\v

\begin{figure}[H]
\includegraphics[scale=0.5]{images/rl.png}
\centering
\end{figure}

\bd[History]
\textbf{History} (at time $t$) $H_t$ is the sequence of states,  actions and rewards from the very beginning up to time $t$:
\bse
H_t = \{S_1, A_1,R_1,S_2, A_2,R_2,\ldots, S_t, A_t,R_t \}
\ese
\ed

Moving on to some more definitions, we turn now our attention to the agent and we define the concept of a ``policy" which, in a way,  determines the agent's behaviour.

\bd[Policy]
A \textbf{policy} $\pi$ is a map from state space to action space and determines the behaviour of the agent in each state:
\bse
\pi : S \to A
\ese
\ed

A policy can be either ``deterministic" or ``stochastic".

\bd[Deterministic Policy]
A \textbf{deterministic policy} is a policy that assigns one action $a$ to every state $s$ with probability 1:
\bse
\pi(s) = a
\ese
\ed

\bd[Stochastic Policy]
A \textbf{stochastic policy} is a policy that assigns a probability distribution over actions $a$ to every state $s$:
\bse
\pi(a|s) = P(A=a|S=s)
\ese
\ed

\bd[Policy-Based]
An agent that tries to learn an optimal policy is called \textbf{policy-based}.
\ed

Having in hand a policy one can define a quantity that ``measures" how good this policy is. This quantity is the so called ``value function" and since it measures the goodness of a specific policy,  it is always attached to a specific policy $\pi$.

\bd[Value Function]
A \textbf{value function} $V_{\pi}$ of a policy $\pi$ is a map from state space to real numbers and it represents a prediction of expected future reward an agent will get by following policy $\pi$:
\bse
V_{\pi} : S \to R
\ese
\ed

A value function gives us the expected future total reward tan an agent will get if it starts form state $s$ and follows policy $\pi$.  In general is hard to use a value function in order to realize how an agent should act.  However it is a very easy task to translate a value function to a corresponding policy and this is the reason why we always assign a policy to a value function.

\bd[Value-Based]
An agent that tries to learn an optimal value function is called \textbf{value-based}.
\ed

\bd[Actor-Critic]
An agent that has both a policy and a value function is called \textbf{actor-critic}.
\ed

A final, crucial part of reinforcement learning problems is the concept of a ``model" of the environment.  In general an agent might have a model of the environment or not.  In order to define the concept of a model we first need two more definitions that we will also see in detail in the next chapters.

\bd[State Transition Probability]
The \textbf{state transition probability} $\mathcal{P}$ provides all the relevant information on the probability of an agent ending up on a successor state given that it started from an initial state.
\ed

\bd[Expected Reward]
The \textbf{expected reward} $\mathcal{R}$ is the expected,, immediate reward an agent will get given current its state and action.
\ed

Both ``state transition probability" and ``expected reward" will be defined again in later chapters with more detail and in the appropriate mathematical framework.  For now we will use them to define the ``model".

\bd[Model]
A \textbf{model} (of the environment) is a tuple $(\mathcal{P},\mathcal{R})$ where $\mathcal{P}$ is the state transition probability and $\mathcal{R}$ is the expected reward.
\ed

\bd[Model-Based]
An agent that has access to a model of the environment is called \textbf{model-based}.
\ed

\bd[Model-Free]
An agent that has not access to a model of the environment is called \textbf{model-free}.
\ed

Depending on if the agent has a policy or a value function (or both) and if it also has a model or not we can end up in four different reinforcement learnings problems:

\bit
\item Model-Free Policy-Based
\item Model Free Value-Based
\item Model-Based Policy-Based
\item Model Based Value-Based
\eit

\bd [Planning]
In the case of model-based reinforcement learning problems where a model of the environment is known,  the agent performs computations with its model (without any external interaction) and then it improves its policy.  We refer to this case as \textbf{planning}.
\ed

\bd[Reinforcement Learning (Reprise)]
In the case of model-free reinforcement learning problems where a model of the environment is initially unknown,  the agent interacts with the environment and then it improves its policy.  We refer to this case as \textbf{reinforcement learning}.
\ed

Reinforcement learning is like trial-and-error learning where the agent should discover a good policy from its experiences of the environment without losing too much reward along the way.  During this process an agent can either ``explore" where it finds more information about the environment or ``exploit" where it uses known information to maximise reward.  As we already mentioned in the introduction,  it is important to explore as well as exploit and that creates the whole trade-off between exploration and exploitation.  \v

As a final note,  in reinforcement learning we distinguish between two situations.

\bd[Prediction]
In \textbf{prediction} one tries to evaluate a given policy.
\ed

\bd[Control]
In \textbf{control} one tries to find an optimal policy.
\ed

The general case is that we first need to solve the prediction problem in order to solve the control problem.

\section{Markov Decision Process}

Markov decision processes (MDPs) formally describe fully observable environments (i.e when the current states completely characterise the processes) for reinforcement learning problems.  It is a general truth that almost all reinforcement learning problems can be formalised as MDPs, since partially observable environment problems can also be converted into MDPs.  A key characteristic in what follows is the concept of ``Markov property".  Let us start by defining it formally.

\bd[Markov Property]
Let $\{X_i\}$ be a stochastic process.  The stochastic process is said to carry the \textbf{Markov property} if and only if:
\bse
P(X_{n}=x_{n} \mid X_{n-1} = x_{n-1}, \dots , X_{1}=x_{1}) = P(X_{n}=x_{n}\mid X_{n-1}=x_{n-1})
\ese
\ed

Markov property refers to the memoryless property of a stochastic process,  since once $X_n$ is known, the rest of the history $\{X_1, X_2, \ldots,X_{n-1}\}$ can be thrown away since $X_n$ itself contains all the information of history needed ($X_n$ is a sufficient statistic of the future).  In other words the future is independent of the past given the present.  Although Markov theory is a standalone mathematical theory completely independent from reinforcement learning, here we will focus solely on reinforcement learning and we will define everything in terms of reinforcement learning.  

In order to make the connection with reinforcement learning,  we define the ``Markov state" as follows.
\bd[Markov State]
A state $S_t$ is called a \textbf{Markov state} if and only if:
\bse
P(S_{t+1}=s_{t+1} \mid S_{t} = s_{t}, \dots , S_{1}=s_{1}) = P(S_{t+1}=s_{t+1} \mid S_{t} = s_{t})
\ese
\ed

The Markov state captures all relevant information from the history and once the state is known,  the history may be thrown away. 

\subsection{Markov Process}

Given the definition of Markov states we are now in a position to start building the mathematical framework of Markov decision processes. The first step towards this direction is the so called ``Markov process". In order to define it, we will first (re)define in detail the ``state transition probability" which we have already introduced in the previous section but for the specific case of a Markov process.

\bd[State Transition Probability (For Markov Process)]
The \textbf{state transition probability} between an initial state $s$ and a successor state $s^\prime$ $\mathcal{P}_{s s^\prime}$ is defined as the probability of ending in state $s^\prime$ given the current state $s$:
\bse
\mathcal{P}_{s s^\prime} = P(S_{t+1}=s^\prime \mid S_{t} = s)
\ese
\ed

It is quite usual to represent the state transition probability in the form of a matrix.

\bd[State Transition Matrix (For Markov Process)]
The \textbf{state transition matrix} $\mathcal{P}$ defines transition probabilities from all states $s$ to all successor states $s^\prime$:
\bse
\mathcal{P} = \begin{vmatrix}
\mathcal{P}_{11} & \mathcal{P}_{12} & \ldots & \mathcal{P}_{1n}\\
\mathcal{P}_{21} & \mathcal{P}_{22} & \ldots & \mathcal{P}_{2n}\\
\vdots & \vdots & \ddots & \vdots  \\
\mathcal{P}_{n1} & \mathcal{P}_{n2} & \ldots & \mathcal{P}_{nn}
\end{vmatrix}
\ese
\ed

\v

Given the state transition probability/matrix we can now define ``Markov processes".

\bd[Markov Process]
A \textbf{Markov process} is a tuple $(S, \mathcal{P}_{s s^\prime})$ where $S$ is a (finite) set of Markov states:
\bse
S = \{S_1,  S_2,  \ldots, S_n\}
\ese

and $\mathcal{P}_{s s^\prime}$ is the state transition probability:
\bse
\mathcal{P}_{s s^\prime} = P(S_{t+1}=s^\prime \mid S_{t} = s)
\ese
\ed

\bd[Terminal State]
A \textbf{terminal state} is a state where the probability of returning to itself in the next step is equal to 1.
\ed

In other words,  once the Markov process lands in a terminal state then it is over, since it cannot ``escape" this state.  

\bd[Episode]
An \textbf{episode} is any sequence of states of a Markov process that starts from an initial state and terminates at a final terminal state.
\ed

Now let's see an example of a Markov process.

\v

\begin{figure}[H]
\includegraphics[scale=0.45]{images/rl1.png}
\centering
\end{figure}

In this example the set of Markov states is:
\bse
S = \{S_1,  S_2, S_3,  S_4,  S_5,  S_6, S_7\}
\ese

and the state transition probability (here we will use the state transition matrix since it's more handy) is:

\begingroup
\renewcommand*{\arraystretch}{1.5}
\bse
\mathcal{P} = \begin{vmatrix}
0 & 0.5 & 0 & 0 & 0 & 0.5 & 0\\
0 & 0 & 0.8 & 0 & 0 & 0 & 0.2\\
0 & 0 & 0 & 0.6 & 0.4 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 1\\
0.2 & 0.4 & 0.4 & 0 & 0 & 0 & 0\\
0.1 & 0 & 0 & 0 & 0 & 0.9 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 1\\
\end{vmatrix}
\ese
\endgroup

\vspace{10pt}

In this specific example the state $S_7$ is a terminal state,  hence a possible episode could be:
\bse
S_1 \to S_2 \to S_3 \to S_4 \to S_7
\ese

\subsection{Markov Reward Process}

A Markov process is not adequate enough for describing reinforcement learning problems.  We will now add some more complexity to a Markov process to end up in the so called ``Markov reward process".  First we will (re)define in detail the ``expected reward" which we have already introduced in the previous section but for the specific case of a Markov decision process. Then we we will define the so called ``discount factor".

\bd[Expected Reward (For Markov Reward Process)]
\textbf{Expected reward} $\mathcal{R}_{s}$ of a state $s$ is the expected, immediate reward an agent will get for being in state $s$:
\bse
\mathcal{R}_{s} = E[R_{t+1} | S_t=s]
\ese
\ed

\bd[Discount Factor]
\textbf{Discount factor} $\gamma$ is a scalar quantity with $\gamma \in [0,1]$ used to discount future rewards.
\ed

In simple words discount factor is used to compute the present value of future rewards.  Now that we have introduced the expected reward and the discount factor we can expand the notion of a Markov process to that of a Markov reward process.

\bd[Markov Reward Process]
A \textbf{Markov reward process} is a tuple $(S,  \mathcal{P}_{s s^\prime},  \mathcal{R}_{s}, \gamma)$ where $S$ is a (finite) set of Markov states:
\bse
S = \{S_1,  S_2,  \ldots, S_n\}
\ese 

$\mathcal{P}_{s s^\prime}$ is the state transition probability:
\bse
\mathcal{P}_{s s^\prime} = P(S_{t+1}=s^\prime \mid S_{t} = s)
\ese

$\mathcal{R}_{s}$ is the expected reward:
\bse
\mathcal{R}_{s} = E[R_{t+1} | S_t=s]
\ese

and $\gamma$ is the discount factor with:
\bse
\gamma \in [0,1]
\ese
\ed

Once we have a Markov reward process we can define the concept of ``return" as follows.

\bd[Return]
\textbf{Return}  $G_t$ is the total discounted reward from time-step $t$ and forward:
\bse
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\ese
\ed

A few comment about the definition of return and the choice of including a discount factor in the definition are in order. Since the discount factor $\gamma \in [0,1] \Rightarrow \gamma^k \to 0$ as $k \to \infty$.  In other words the discount factor weights more the near future and less the distant future.  This is why the discount factor acts as the present value of future rewards.  In general most Markov reward (and decision) processes are discounted.  This is happening for many reasons. First of all it is mathematically convenient to discount rewards.  Secondly we avoid infinite returns in cyclic Markov processes.  Thirdly,  uncertainty about the future may not be fully represented. Finally, animal/human behaviour shows preference for immediate rewards. \v

In one extreme case where only the immediate reward is important we can set $\gamma = 0$ and then we simply obtain $G_t = R_{t+1}$.  This is called  ``myopic" evaluation.  It is also possible to use undiscounted Markov reward processes $\gamma = 1$ where all future rewards contribute equally. This is called ``far-sighted" evaluation.  In most of the cases we pick a value for $\gamma$ between those extreme cases.  \v

Given the return $G_t$ we can now  (re)define the ``state value function" which we have already introduced in the previous section but for the specific case of a Markov decision process. 

\bd[State Value Function (For Markov Reward Process)]
The \textbf{state value function } $V(s)$ of a Markov reward process is the expected return starting from state $s$:
\bse
V(s) = E [G_t | S_t = s]
\ese
\ed

In simple words,  state value function informs us about the expected return an agent will obtain by starting from a state $s$ by taking an expected value over all possible episodes that start from state $s$.  Notice that in the case of myopic evaluation ($\gamma=0$) the value state function is equal to the expected reward since this is the only state that counts in any episode (for $\gamma=0$):
\bse
V(s) = E [G_t | S_t = s] = E [R_{t+1} | S_t = s] = \mathcal{R}_{s} 
\ese

For values of $\gamma$ different than 0 the state value function takes into account future expected rewards as well.  \v

We can manipulate the value function in order to decompose it into two parts: the expected reward and the discounted state value function of all possible successor states as follows:
{\setlength{\jot}{10pt}
\begin{align*}
V(s) &= E [G_t | S_t = s] \\
&= E [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots | S_t = s] \\
&= E [R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \ldots) | S_t = s] \\
&= E [R_{t+1} + \gamma G_{t+1}  | S_t = s] \\
&= E [R_{t+1}| S_t = s]  + E[\gamma G_{t+1}  | S_t = s] \\
&= \mathcal{R}_{s}   + \gamma E[G_{t+1}  | S_t = s]
\end{align*}}

\vspace{-10pt}

Where in the last equality we used the definition of the reward function $\mathcal{R}_{s}$.  By using the law of conditional expected value (check probability notes for details) we obtain:
\bse
V(s) = \mathcal{R}_{s}   + \gamma \sum_{s^\prime} E[G_{t+1}  | S_t = s,  S_{t+1} = s^\prime] \cdot P(S_{t+1} = s^\prime | S_t = s)
\ese

Finally by using the Markov property and the definition of state transition probability:
{\setlength{\jot}{10pt}
\begin{align*}
V(s) &= \mathcal{R}_{s}   + \gamma \sum_{s^\prime} E[G_{t+1} | S_{t+1} = s^\prime)   \cdot \mathcal{P}_{ss^\prime}   \\
&= \mathcal{R}_{s}   + \gamma \sum_{s^\prime} \mathcal{P}_{ss^\prime} V(s^\prime) 
\end{align*}}

\vspace{-10pt}

This final expression is called ``Bellman's equations" and it is used in order to derive the state value function for all states of a Markov reward process.  It simply states that the expected return for a state $s$ (aka the state value function) is equal to the expected reward an agent will get just for being in state $s$ plus the expected rewards of all possible successive states weighted by the probability of reaching them and discounted by the discount factor $\gamma$. 

\begin{figure}[H]
\includegraphics[scale=0.55]{images/rl4.png}
\centering
\end{figure}

\v

By defining the following vectors:
\bse
\boldsymbol{V} = \begin{bmatrix} V(S_1) \\[1ex] V(S_2) \\[1ex] \vdots \\[1ex] V(S_n) \end{bmatrix}, \qquad
\boldsymbol{\mathcal{R}} = \begin{bmatrix} \mathcal{R}_{S_1} \\[1ex] \mathcal{R}_{S_2} \\[1ex] \vdots \\[1ex] \mathcal{R}_{S_n} \end{bmatrix}
\ese

\v

we can write Bellman's equations in a matrix form as:
\bse
\boldsymbol{V} = \boldsymbol{\mathcal{R}} + \gamma \mathcal{P} \boldsymbol{V} 
\ese

where $\mathcal{P}$ is the state transition matrix. \v

Since the equation is linear we can solve it directly (as we did in the case of Normal equation in supervised learning) and obtain a solution for $\boldsymbol{V}$.  Namely:
{\setlength{\jot}{5pt}
\begin{align*}
& \boldsymbol{V} = \boldsymbol{\mathcal{R}} + \gamma \mathcal{P} \boldsymbol{V}  \Rightarrow \\
& \boldsymbol{V}  -  \gamma \mathcal{P} \boldsymbol{V}  = \boldsymbol{\mathcal{R}} \Rightarrow \\
& (I  -  \gamma \mathcal{P}) \boldsymbol{V}  = \boldsymbol{\mathcal{R}} \Rightarrow \\
& (I  -  \gamma \mathcal{P})^{-1} (I  -  \gamma \mathcal{P}) \boldsymbol{V}  = (I  -  \gamma \mathcal{P})^{-1} \boldsymbol{\mathcal{R}} \Rightarrow \\
& \boldsymbol{V}  = (I  -  \gamma \mathcal{P})^{-1} \boldsymbol{\mathcal{R}}
\end{align*}}

\vspace{-12pt}

As in supervised learning with normal equation, when the number of states $n$ is large the matrix $\mathcal{P}$ gets large and computing the inverse of it, is computationally very expensive (and in some cases even impossible).  One of the main goals of reinforcement learning is to come up with algorithms of solving Bellman's equations indirectly. We will come back to this once we have developed the full framework needed for reinforcement learning problems. \v

Now let's see an example of a Markov reward process.

\begin{figure}[H]
\includegraphics[scale=0.45]{images/rl2.png}
\centering
\end{figure}

As we can see,  the only difference with the Markov process example is that now in Markov decision process each state carries a reward $R$.  In order to practise let's calculate the state value function for each state through the Bellman's equations.   \v

For the first state:
\begin{align*}
V(S_1) &= \mathcal{R}_{s_1}   +  \gamma \sum_{s^\prime} \mathcal{P}_{s_1 s^\prime} V(s^\prime) \\
&=  \mathcal{R}_{s_1}   + 0.5 \cdot \mathcal{P}_{s_1 s^\prime}  \cdot \sum_{s^\prime} V(s^\prime) \\
\end{align*}

For the expected reward  $\mathcal{R}_{s_1}$ we have:
{\setlength{\jot}{5pt}
\begin{align*}
\mathcal{R}_{s_1} &= E[R_{t+1} | S_t=s_1] \\
&=  \sum_{s^\prime} E[R_{t+1} | S_t=s_1,  S_{t+1} = s^\prime] \cdot P(S_{t+1} = s^\prime | S_t = s_1) \\
&=  \sum_{s^\prime} E[R_{t+1} | S_{t+1} = s^\prime] \cdot P_{s_1 s^\prime} \\
&=  \sum_{s^\prime} R_{s^\prime} \cdot P_{s_1 s^\prime} \\
&= R_{s_1} \cdot P_{s_1 s_1} +  R_{s_2} \cdot P_{s_1 s_2} +  R_{s_3} \cdot P_{s_1 s_3} +  R_{s_4} \cdot P_{s_1 s_4} +  R_{s_5} \cdot P_{s_1 s_5} +  R_{s_6} \cdot P_{s_1 s_6} +  R_{s_7} \cdot P_{s_1 s_7}  \\
&= (-2) \cdot 0 +  (-2) \cdot 0.5 +  (-2) \cdot 0 +  (10) \cdot 0 + (1) \cdot 0 +  (-1) \cdot 0.5 +  (0) \cdot 0 \\
&= -1.5
\end{align*}}

For the state value function part we have:
{\setlength{\jot}{5pt}
\begin{align*}
\gamma \sum_{s^\prime} \mathcal{P}_{s_1 s^\prime} V(s^\prime) &= R_{s_1} \cdot P_{s_1 s_1} +  R_{s_2} \cdot P_{s_1 s_2} +  R_{s_3} \cdot P_{s_1 s_3} +  R_{s_4} \cdot P_{s_1 s_4} +  R_{s_5} \cdot P_{s_1 s_5} +  R_{s_6} \cdot P_{s_1 s_6} +  R_{s_7} \cdot P_{s_1 s_7}  \\
\end{align*}

As an an example one could compute the return of the episode we mentioned earlier:
\bse
S_1 \to S_2 \to S_3 \to S_4 \to S_7
\ese 

By choosing $\gamma=\frac{1}{2}$ we have:
\bse
G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} 
\ese

\subsection{Markov Decision Process}

Once again a Markov reward process is not adequate enough for describing reinforcement learning problems since there is no notion of actions that an agent can take.  We will now add a final piece of complexity to a Markov reward process by introducing ``action" to end up in the so called ``Markov decision process". \v

The starting point is the concept of an ``action" as it was introduced earlier, i.e  is an action $A$ is an action taken by an agent at each state $S$.  In reinforcement learning we always make the assumption that at each state the agent has a finite amount of possible actions it can perform.  Once we have defined the actions we can now define a ``Markov decision process".

\bd[Markov Decision Process]
Given a memoryless discrete stochastic process of random Markov states $\{S_0, S_1, S_2, \ldots,S_n\}$,  a \textbf{Markov decision process} is a tuple $(S,A,\mathcal{P}, \mathcal{R}, \gamma)$ where $S$ is a (finite) set of states, $A$ is finite set of actions,  $\mathcal{P}$ is a state transition probability matrix with:
\bse
\mathcal{P}^{a}_{s s^\prime} = P(S_{t+1} = s^\prime | S_t=s, A_t = a)
\ese  

$\mathcal{R}$ is a reward function with:
\bse
\mathcal{R}^{a}_{s} = E[R_{t+1} | S_t=s, A_t = a]
\ese

and $\gamma$ is a discount factor.
\ed