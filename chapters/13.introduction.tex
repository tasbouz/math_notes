\bd[Machine Learning]
\textbf{Machine learning} is the field of study that gives computers the ability to learn without being explicitly programmed. (Arthur Samuel, 1969)
\ed

\bd[Machine Learning]
A computer program is said to learn from experience $E$ with respect to some task $T$ and some performance measure $P$, if its performance on $T$, as measured by $P$, improves with experience $E$. (Tom Mitchell)
\ed

The essence of machine learning is that a pattern exists and it can not be pined down mathematically, however we have data on it and we can treat it in a probabilistic way.\v

Thus,  machine learning is great for:
\bit
\item Problems for which existing solutions require a lot of fine-tuning or long lists of rules: one machine learning algorithm can often simplify code and perform better than the traditional approach.
\item Complex problems for which using a traditional approach yields no good solution: the best machine learning techniques can perhaps find a solution.
\item Fluctuating environments since a machine learning system can adapt to new data.
\eit

Some usual examples of machine learning algorithms are: analyzing images to automatically classify them, detecting tumors in brain scans,  automatically classifying news articles,  automatically flagging offensive comments on discussion forums,  creating a chat-bot or a personal assistant,  forecasting company revenue,  making an application reacting to voice commands, detecting credit card fraud,  segmenting clients based on their purchases, representing a complex high-dimensional dataset in a clear and insightful way,  recommending a product that a client may be interested in,  building an intelligent bot for a game,  and many, many more...\v

There are some common conventions in the machine learning community around the notation used to describe various notions.  We will of course follow the same conventions. In order to briefly formalize the essence of machine learning we will introduce some of the very basic notation that we will be using throughout the notes now, although we will introduce more notation in the later chapters.  Here are some very basic concepts with their usual notation:
\bit
\item Input: $x \in X$.
\item Output: $y \in Y$.
\item Data:  $\{ x_{i}, y_{i} \}, \:\:\: i=1,2,3,\ldots, m$.
\item Target Function: $f: X \to Y$.
\item Hypothesis Function: $h: X \to Y$ with $ h \approx f$.
\item Hypothesis Set: $H = \{h\}$.
\eit

Informally, the goal of machine learning is, based on the data $\{ x_{i}, y_{i} \}$,  to discover a hypothesis function $h$, out of a set of possible hypothesis functions $H$, that behaves in a similar way with the target function $f$ which is, and always will be, unknown to us. 

\vspace{10pt}

\begin{figure}[H]
\includegraphics[scale=0.4]{images/mlmodel.png}
\centering
\end{figure}

\vspace{10pt}

The question is how can we learn an unknown function $f$ just based on the data we already have, when the unknown function $f$ in general can take any value outside the known data. The short answer is that we can not however, without proving it, the following relation holds:
\bse
P \Big[ | E_{\text{in}} (h) - E_{\text{out}} (h) |  > \epsilon \Big] \leq 2 \cdot M \cdot e^{2\epsilon^2m}
\ese

where $ E_{\text{in}} (h)$ is the error that we get for $h$ in the known data, $E_{\text{out}} (h)$ is the error that we will get when we use $h$ for new data, $M$ is the number of possible hypothesis function $h$ (i.e the cardinality of the hypothesis set $H = \{h\}$, $\epsilon$ is the tolerance that we have for errors, and $m$ is the number of data points. This equation tells us that no matter what, learning is possible only in a probabilist sense. We will always have an error, since the whole process carries a stochastic nature.\v

So we can informally summarize what we are trying to do with machine learning as:
\bit
\item From aforementioned relation: $E_{\text{in}} \approx E_{\text{out}}$.
\item From learning algorithm:  $E_{\text{in}} \approx 0$.
\item From the combination of these 2: $E_{\text{out}} \approx 0$.
\eit

By having $E_{\text{out}} \approx 0$, that means that our hypothesis function $h$ generalizes well for out of sample data, so we can use it for predictions. That in a nutshell is how machine learning works. \v

There are so many different types of machine learning systems that it is useful to classify them in broad categories,  based on the following criteria:
\begin{enumerate}
\item Whether or not they are trained with human supervision.  Based on this category we have the following subcategories:
	\bit
	\item \textbf{Supervised Learning}
	
In supervised learning the training set you feed to the algorithm includes the desired solutions,  called labels.  Some of the most important supervised learning algorithms are: Linear Regression, Logistic Regression, Support Vector Machines (SVMs), k-Nearest Neighbors, Decision Trees, Random Forests, Neural Networks.
	\item \textbf{Unsupervised Learning}
	
In unsupervised learning the training data is unlabeled so the system tries to learn without a teacher.  Some of the most important unsupervised learning algorithms are: 
		\bit
		\item Clustering algorithms like K-Means,  DBSCAN and Hierarchical Cluster Analysis (HCA).
		\item Anomaly detection algorithms such as One-Class SVM and Isolation Forest.
		\item Dimensionality reduction algorithms such as Principal Component Analysis (PCA),  Kernel PCA, Locally Linear Embedding (LLE),  and t-Distributed Stochastic Neighbor Embedding (t-SNE).
		\eit
	\item \textbf{Semisupervised Learning}
	
In semisupervised learning one has plenty of unlabeled instances,  and few labeled instances.  Most semisupervised learning algorithms are combinations of unsupervised and supervised algorithms.  
	\item \textbf{Reinforcement Learning}
	
In reinforcement learning the learning system,  called an agent, can observe the environment,  select and perform actions,  and get rewards in return. It must then learn by itself what is the best strategy,  called a policy,  to get the most reward over time.  A policy defines what action the agent should choose when it is in a given situation. 
	\eit
\item Whether or not they can learn incrementally on the fly.  Based on this category we have the following subcategories:
	\bit
	\item \textbf{Offline or Batch Learning}
	
In offline,  or batch, leaning the system is incapable of learning incrementally and it must be trained using all the available data.  This will generally take a lot of time and computing resources, so it is typically done offline.  First the system is trained,  and then it is launched into production and runs without learning anymore, it just applies what it has learned. 
	\item \textbf{Online Learning}: 
	
In online learning one trains the system incrementally by feeding it data instances sequentially, either individually or in small groups called ``mini-batches".  Each learning step is fast and cheap, so the system can learn about new data on the fly, as it arrives.
	\eit
\item Whether they work by simply comparing new data points to known data points,  or instead by detecting patterns in the training data and building a predictive model.  Based on this category we have the following subcategories:
	\bit
	\item \textbf{Instance-Based Learning}
	
In instance-based learning the system learns the examples by heart, then generalizes to new cases by using a similarity measure to compare them to the learned examples (or a subset of them). 
	\item \textbf{Model-Based Learning}

In model-based learning one in order to generalize from a set of examples they build a model of these examples and then use that model to make predictions. 
	\eit
\end{enumerate} 

These criteria are not exclusive.  You can combine them in any way you like.  This fact makes machine learning is a very broad topic with many different branches and applications.  In these notes we will cover the vast majority of them.