Supervised learning is one of the four basic categories of machine learning, and it consists of a family of models and techniques that we will introduce in this chapter. First let's start with a formal definition of supervised learning.

\bd[Supervised Learning]
\textbf{Supervised learning} is the machine learning task of learning a function that maps an input to an output based on examples of \say{input - output} pairs called a \say{training set}.
\ed

\begin{figure}[H]
\includegraphics[scale=0.5]{images/supervisedmodel.png}
\centering
\end{figure}

Some more specific notation that we will be using throughout supervised learning:
\bit
\item Input variables or attributes or features: $x$
\item The i'th feature (in case of many features): $x_{i}$
\item The i'th training example (in case of many training examples): $x^{(i)}$
\item The i'th feature of the  j'th training example: $x_{i}^{(j)}$
\item Output variables or targets or classes: $y$
\item The i'th output target: $y^{(i)}$
\item Total number of training examples: $m$
\item Total number of features: $n$\\
\eit

Now let's dive in the models and techniques of supervised learning, starting with one of the most basic ones called \say{linear regression}.

\section{Linear Regression}

\bd[Linear Regression]
\textbf{Linear regression} is a linear approach to modelling the relationship between a dependent variable (target) and one or more independent variables (features).
\ed

\bd[Simple \& Multiple Linear Regression]
When there is only one independent variable then the model is called \textbf{simple linear regression}. For more than one independent variable, the process is called \textbf{multiple linear regression}.
\ed

\bd[Univariate \& Multivariate Linear Regression]
When only one dependent variable is predicted then the model is called \textbf{univariate linear regression}. For more than one correlated, dependent variables being predicted, the process is called \textbf{multivariate linear regression}.
\ed

In linear regression the hypothesis function $h$ is a linear combinations of the features:
\bse
h(x) =  w_0 + w_{1} x_1 + \ldots + w_n x_n
\ese

where $w_0$ is is called \say{bias} and $w_i$'s  are called \say{weights}. We usually refer to weights and bias as the \say{parameters} of the regression and they are the ones that we try to determine through the training examples by using a learning algorithm. Once we find them then $h$ is ready to predict new inputs with unknown outcomes.

\begin{figure}[H]
\includegraphics[scale=0.3]{images/linearregression.png}
\centering
\end{figure}

\vspace{5pt}

It is a usual procedure to define $x_0 = 1$, so linear regression the hypothesis function can be rewritten as
\bse
h(x) = w_0 x_0 + w_1 x_1 + \ldots + w_n x_n = \sum_{i=0}^{n} w_i x_i
\ese

Moreover by defining the feature vector $\boldsymbol{x}$ and parameter vector $\boldsymbol{w}$ as
\bse
\boldsymbol{x} = \begin{bmatrix} x_{0} \\ x_{1} \\ \vdots \\ x_{n} \end{bmatrix}, \qquad
\boldsymbol{w} = \begin{bmatrix} w_{0} \\ w_{1} \\ \vdots \\ w_{n} \end{bmatrix}
\ese

\vspace{3pt}

then we can rewrite the linear regression the hypothesis function in a very simple form of:
\bse
h(x) = \boldsymbol{w}^T \boldsymbol{x}
\ese

Now that we have a hypothesis function, we need a rule in order to be able to find the parameters $\boldsymbol{w}$. This rule can be obtained through the probabilistic interpretation of linear regression.\\

More precisely, after having obtained the parameters $\boldsymbol{w}$, the hypothesis will fit the data in the best possible way but, since as we said we are dealing with probabilistic systems, we will still have some errors $\epsilon$. In other words, for each training example the following formula will apply:
\bse
y^{(i)} = h(x^{(i)}) + \epsilon^{(i)} = \boldsymbol{w}^T \boldsymbol{x}^{(i)} + \epsilon^{(i)}
\ese

where $\boldsymbol{x}^{(i)}$ is the corresponding training example feature vector:
\bse
\boldsymbol{x}^{(i)} = \begin{bmatrix} x_{0}^{(i)} \\ x_{1}^{(i)} \\ \vdots \\ x_{n}^{(i)} \end{bmatrix}
\ese

\vspace{5pt}

At this point we will make one assumption which needs to be valid in order for the linear regression to be valid. Namely, we assume that the errors $ \epsilon^{(i)}$ are independent and identically distributed following a normal distribution with mean 0 and variance $\sigma^2$:
\bse
\epsilon^{(i)} \sim N(0, \sigma^2)
\ese

which means that the probability distribution of the errors is given by
\bse
P( \epsilon^{(i)}) = \frac{1}{\sqrt{2 \pi \sigma^2}} exp \Big( - \frac{(\epsilon^{(i)})^{2}}{2 \sigma^2} \Big)
\ese

From the assumption of the errors follows
\begin{align*}
&y^{(i)} = \boldsymbol{w}^T \boldsymbol{x}^{(i)} + \epsilon^{(i)} \Rightarrow \\ &\epsilon^{(i)} = y^{(i)} - \boldsymbol{w}^T \boldsymbol{x}^{(i)} 
\end{align*}

By substituting the error back to the probability
{\setlength{\jot}{10pt}
\begin{align*}
&P( \epsilon^{(i)}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \cdot exp \Big( - \frac{(\epsilon^{(i)})^2}{2 \sigma^2} \Big) \Rightarrow \\
& P(y^{(i)} | \boldsymbol{x}^{(i)} ; \boldsymbol{w}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \cdot exp \Big( - \frac{(y^{(i)} - \boldsymbol{w}^T \boldsymbol{x}^{(i)})^2}{2 \sigma^2} \Big)
\end{align*}}

In other words we get that the conditional distribution of $y^{(i)}$ given $\boldsymbol{x}^{(i)}$ and $\boldsymbol{w}$ is a normal distribution with mean $\boldsymbol{w}^T \boldsymbol{x}^{(i)}$ and variance $\sigma^2$
\bse
y^{(i)} \sim N(\boldsymbol{w}^T \boldsymbol{x}^{(i)}, \sigma^2)
\ese

\vspace{3pt}

Given the probability distribution of $y^{(i)}$ as a function of the parameters, we can now use the principle of maximum likelihood that we developed in parametric inference chapter, in order to find the rule that will give us the best parameters $\boldsymbol{w}$. \\

For the likelihood it is
\bse
\mathcal{L} (\boldsymbol{w} |y ) = P(y^{(1)}, y^{(2)}, \ldots, y^{(m)} | \boldsymbol{x}^{(1)}, \boldsymbol{x}^{(2)}, \ldots, \boldsymbol{x}^{(m)} ; \boldsymbol{w}) = \prod_{i=1}^{m} P(y^{(i)} | \boldsymbol{x}^{(i)} ; \boldsymbol{w})
\ese

where we used the fact that $\epsilon^{(i)}$ are independent. By substituting the probability
\bse
\mathcal{L} (\boldsymbol{w} |y )  = \prod_{i=1}^{m} \frac{1}{\sqrt{2 \pi \sigma^2}} \cdot exp \Big( - \frac{(y^{(i)} - \boldsymbol{w}^T \boldsymbol{x}^{(i)})^2}{2 \sigma^2} \Big)
\ese

Subsequently for the log-likelihood
{\setlength{\jot}{10pt}
\begin{align*}
l (\boldsymbol{w} |y ) &= \ln \mathcal{L} (\boldsymbol{w} |y ) \\ 
&= \ln \Big[ \prod_{i=1}^{m} \frac{1}{\sqrt{2 \pi \sigma^2}} \cdot exp \Big( - \frac{(y^{(i)} - \boldsymbol{w}^T \boldsymbol{x}^{(i)})^2}{2 \sigma^2} \Big) \Big] \\
&= \sum_{i=1}^{m} \ln \Big[ \frac{1}{\sqrt{2 \pi \sigma^2}} \cdot exp \Big( - \frac{(y^{(i)} - \boldsymbol{w}^T \boldsymbol{x}^{(i)})^2}{2 \sigma^2} \Big) \Big] \\
&= \sum_{i=1}^{m} \ln \Big[ \frac{1}{\sqrt{2 \pi \sigma^2}} \Big] +  \sum_{i=1}^{m} \ln \Big[ exp \Big( - \frac{(y^{(i)} - \boldsymbol{w}^T \boldsymbol{x}^{(i)})^2}{2 \sigma^2} \Big] \Big) \\
&= \sum_{i=1}^{m} \ln \Big[ \frac{1}{\sqrt{2 \pi \sigma^2}} \Big] +  \sum_{i=1}^{m} \Big[ - \frac{(y^{(i)} - \boldsymbol{w}^T \boldsymbol{x}^{(i)})^2}{2 \sigma^2} \Big]
\end{align*}}

According to the principle of maximum likelihood, the best parameters can be found by maximizing the log-likelihood. The first term of the log-likelihood is just a constant term so it does not contribute at all to the maximization, and the same holds for the denominator of the second term. Hence:
{\setlength{\jot}{10pt}
\begin{align*}
\boldsymbol{w} &= \argmax_{\boldsymbol{w}} [ l (\boldsymbol{w} |y )] \\
&= \argmax_{\boldsymbol{w}} \Big[ \sum_{i=1}^{m} ( - (y^{(i)} - \boldsymbol{w}^T \boldsymbol{x}^{(i)})^2) \Big] \\
&= \argmax_{\boldsymbol{w}} \Big[ - \sum_{i=1}^{m} (y^{(i)} - \boldsymbol{w}^T \boldsymbol{x}^{(i)})^2) \Big]\\
&= \argmin_{\boldsymbol{w}} \Big[ \sum_{i=1}^{m} (y^{(i)} - \boldsymbol{w}^T \boldsymbol{x}^{(i)})^2 \Big]
\end{align*}}

At this point we can formally define the following function:

\bd [Mean Squared Error Loss Function]
\textbf{Mean squared error loss function} (MSE) $J(\boldsymbol{w})$ is defined as:
\bse
J(\boldsymbol{w}) = \frac{1}{2m} \sum_{i=1}^{m} (y^{(i)} - \boldsymbol{w}^T \boldsymbol{x}^{(i)})^2
\ese
\ed

\vspace{5pt}

Hence, the principle of maximum likelihood  translates to finding the parameters $\boldsymbol{w}$ that minimize the MSE loss function. The intuition behind the minimization of the MSE loss function is straight forward since what we are actually doing is minimizing the square of the errors between the prediction and the actual outcome (square because only the magnitude of the error is important and not the sign).  By minimizing as much as possible the errors we will eventually get the best line that fits the data.\\

As a final note, we can group together all training examples in one matrix and all training labels in one vector as follows:
\bse
X = \begin{bmatrix} x_{0}^{(1)} & x_{1}^{(1)} & \ldots & x_{n}^{(1)} \\ 
x_{0}^{(2)} & x_{1}^{(2)} & \ldots & x_{n}^{(2)} \\ 
\vdots & \vdots & \ddots & \ldots \\
x_{0}^{(m)} & x_{1}^{(m)} & \ldots & x_{n}^{(m)} 
\end{bmatrix}, \qquad
\boldsymbol{y} = \begin{bmatrix} y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(m)} \end{bmatrix} 
\ese

\vspace{10pt}

By doing so then we can write the MSE loss function in the simple form of:
\bse
J(\boldsymbol{w}) = \frac{1}{2m} (X \boldsymbol{w} - \boldsymbol{y})^2 = \frac{1}{2m} (X \boldsymbol{w} - \boldsymbol{y})^T (X \boldsymbol{w} - \boldsymbol{y})
\ese

\section{Optimization Techniques}

Given the MSE loss function (or any other loss function that we will introduce later on), the goal of machine learning is to optimize it (usually minimize it) in order to obtain the best parameters that fit the data. Optimizing loss functions is one of the biggest parts of machine learning and we can do so with the so called \say{optimization techniques}.

\bd [Optimization Techniques]
\textbf{Optimization techniques} are techniques used for finding the optimum solution or unconstrained maxima or minima of continuous and differentiable functions. These are analytical methods and make use of differential calculus in locating the optimal solution.
\ed

\subsection{Normal Equation}

Probably the most straight forward optimization technique is the so called \say{normal equation}. Since we are looking a minimum for $J(\boldsymbol{w})$ the natural thing to do, is to simply calculate the derivative with respect to the parameter vector and then set it to zero (as we did when we introduced the principle of maximum likelihood).\\

It is more handy to use the vector form of loss function, so for the derivative we get:
{\setlength{\jot}{10pt}
\begin{align*}
\nabla_{\boldsymbol{w}} J(\boldsymbol{w}) &= \frac{1}{2m} \nabla_{\boldsymbol{w}} \Big[ (X \boldsymbol{w} - \boldsymbol{y})^T (X \boldsymbol{w} - \boldsymbol{y}) \Big] \\
&= \frac{1}{2m} \nabla_{\boldsymbol{w}} \Big[ \Big( (X \boldsymbol{w})^T - \boldsymbol{y}^T \Big) \Big( X \boldsymbol{w} - \boldsymbol{y} \Big) \Big] \\
&= \frac{1}{2m} \nabla_{\boldsymbol{w}} \Big[ (X \boldsymbol{w})^T (X \boldsymbol{w}) - (X \boldsymbol{w})^T \boldsymbol{y} - \boldsymbol{y}^T (X \boldsymbol{w}) + \boldsymbol{y}^T \boldsymbol{y} \Big] \\
&= \frac{1}{2m} \nabla_{\boldsymbol{w}} \Big[ (X \boldsymbol{w})^T (X \boldsymbol{w}) - 2 (X \boldsymbol{w})^T \boldsymbol{y} + \boldsymbol{y}^T \boldsymbol{y} \Big] \\
&= \frac{1}{2m} \nabla_{\boldsymbol{w}} \Big[ \boldsymbol{w}^T X^T X \boldsymbol{w} - 2 \boldsymbol{w}^T X^T \boldsymbol{y} + \boldsymbol{y}^T \boldsymbol{y} \Big] \\
&= \frac{1}{2m} \Big[ 2  X^T X \boldsymbol{w} - 2 X^T \boldsymbol{y} \Big] \\
&= \frac{1}{m} \Big[ X^T X \boldsymbol{w} - X^T \boldsymbol{y} \Big]
\end{align*}}

By setting the derivative to 0 we obtain:
{\setlength{\jot}{10pt}
\begin{align*}
& \nabla_{\boldsymbol{w}} J(\boldsymbol{w})  = 0  \Rightarrow \\
& \frac{1}{m} \Big[ X^T X \boldsymbol{w} - X^T \boldsymbol{y} \Big] = 0 \Rightarrow \\
& X^T X \boldsymbol{w} - X^T \boldsymbol{y} = 0 \Rightarrow \\
& X^T X \boldsymbol{w} = X^T \boldsymbol{y} \Rightarrow \\
& \underbrace{(X^T X)^{-1} (X^T X)}_{I} \boldsymbol{w} = (X^T X)^{-1} X^T \boldsymbol{y} \Rightarrow \\
& \boldsymbol{w} = (X^T X)^{-1} X^T \boldsymbol{y}
\end{align*}}

This final expression is called \say{Normal Equation}, and it is an exact analytical solution that gives the parameter vector.\\

Despite the fact that normal equation gives an exact analytical result, computing the seemingly harmless inverse of an $(m \times (n+1)) \times ((n+1) \times m)$ matrix is, with today's most efficient computer science algorithm, of cubic time complexity! This means that as the dimensions of $X$ increase, the amount of operations required to compute the final result increases in a cubic trend. If $X$ was rather small, then using the normal equation would be feasible.\\

In practise, for the vast majority of any industrial application with large datasets, the normal equation would take extremely, sometimes nonsensically, long. This is the reason why normal equation is almost never used. Now let's move on the the most standard optimization technique used today called \say{gradient descent}.

\subsection{Gradient Descent}

Gradient descent, and all its improvements and alternatives, is the most used optimization technique in machine learning and deep learning. In gradient descent we begin with some random initial values for the parameters and we calculate the value the loss function based on them. Then we update them based on the following relation:
\bse
\boldsymbol{w} \coloneqq \boldsymbol{w} - \alpha \nabla_{\boldsymbol{w}} J(\boldsymbol{w})
\ese

Since the derivative is positive when $J$ is upwards slopping and negative when it is downwards slopping, the minus sign makes sure that we always update the parameters towards the direction that minimizes $J$. Once the minimum is reached then $J$ is at a global optimum so the derivative is 0 and further updates are not possible. At this stage the gradient descent is over and the best parameters have been found.

\vspace{-12pt}

\begin{figure}[H]
\includegraphics[scale=0.25]{images/gradientdescent.png}
\centering
\end{figure}

\vspace{5pt}

The hyperparameter $\alpha$ is called \say{learning rate} and defines how big or small steps we take after each iteration of gradient descent. If  $\alpha$ is too large, we might fail to find the minimum due to oscillations around it. If $\alpha$ is too small then gradient descent might take too much time to reach the minimum of $J$. Tuning learning rate in a \say{right} value is a topic by itself and it is quite have researched today!

\begin{figure}[H]
\includegraphics[scale=0.7]{images/alpha.png}
\centering
\end{figure}

\vspace{5pt}

Coming back to our case, let's find the update rule specifically for linear regression. The only thing missing is the derivative of $J$.  However in the previous chapter with normal equation we showed that:
\bse
J(\boldsymbol{w}) = \frac{1}{m} \Big(X^T X \boldsymbol{w} - X^T \boldsymbol{y} \Big) = \frac{1}{m} X^T \Big( X \boldsymbol{w} -\boldsymbol{y} \Big)
\ese

\vspace{5pt}

Hence the update rule reads:
\bse
\boldsymbol{w} \coloneqq \boldsymbol{w} - \frac{\alpha}{m} X^T \Big( X \boldsymbol{w} -\boldsymbol{y} \Big)
\ese

\vspace{5pt}

As we already mentioned there are many improvements and modified algorithms based on gradient descent philosophy. We are going to cover a lot of them in these notes. For now, let's start with 3 basics versions of gradient descent.

\bit
\item Batch Gradient Descent:

Batch gradient descent is actually the one we just saw. As we see in gradient descent, the whole training set $X$ is used in order to make just one update of the parameters. We usually refer to the whole training set as the \say{batch}. For that reason, the usual terminology for what we have seen so far is  \say{batch gradient descent}, meaning that the whole batch is used to update the parameter. 

\item Mini-Batch Gradient Descent:

In \say{mini-batch gradient descent} we divide the whole dataset to $b$ subsets of $\frac{m}{b}$ training examples each, called \say{mini-batches}, and we update the parameters using each of the mini-batches in each iteration.

\item Stochastic Gradient Descent:

In \say{stochastic gradient descent} we only use one training example per time to update the parameters. In every iteration we pick the training example randomly hence the naming.
\eit

One of the main problems of batch gradient descent is that as the number of training examples grows the dimensions of $X$ grows and using the whole training set for every iteration becomes computationally expensive. Both mini-batch and stochastic gradient descent deal with this problem.\\

In a way, mini-batch gradient descent tries to strike a balance between the goodness of batch gradient descent and speed of stochastic gradient descent. In general, batch gradient descent works just fine so we don't need the alternative techniques we just introduced. However in more complicated models, such as deep learning models, these techniques can be really useful. For this reason we will examine again these techniques in more details in the chapter of deep learning.

\section{Logistic Regression}

\bd[Logistic Regression]
\textbf{Logistic regression} is a statistical model that is used to model a binary dependent variable (usually 0 or 1).
\ed

Sometimes logistic regression is called \say{binary classification} since we try to decide if an input belongs to class 0 or class 1 of the binary output. Wecan also generalize to more than 2 discrete classes where then we have \say{multi-class classification}. For now we will focus on binary outputs.

\vspace{5pt}

\begin{figure}[H]
\includegraphics[scale=0.5]{images/classification.png}
\centering
\end{figure}

\vspace{10pt}

Since the output is binary and can take only the values 0 or 1, the hypothesis function of the linear regression is not a valid approximator for logistic regression since it produces a continuous set of outputs. So our first step is to find a suitable hypothesis function $h$ for logistic regression. The hypothesis function that we actually use in logistic regression is the sigmoid function and it is given by:
\bse
h(\boldsymbol{x}) = \frac{1}{1 + exp(- \boldsymbol{w}^T \boldsymbol{x})}
\ese

(From now on, in order to save space, we will write $h(\boldsymbol{x})$ instead of the actual expression for the sigmoid function.)

\vspace{5pt}

\begin{figure}[H]
\includegraphics[scale=0.16]{images/sigmoid.png}
\centering
\end{figure}

\vspace{10pt}

Sigmoid function produces results in the interval $[0,1]$. It is quite close to what we need, but not exactly so, since we do not need all the values between 0 and 1. For this reason we will try to give a different meaning to the hypothesis function.\\

Namely the intuition is that the sigmoid function will act as a probability measure of the target to belong to class 1. The closest to 0 the sigmoid function, the more unlikely for the target to belong in class 1 (hence it belongs to class 0) and the closest to 1 the more likely to belong to the class 1. Hence, by defining a threshold (say at 0.5) the idea is that for $h(\boldsymbol{x}) <0.5$ the algorithm will predict 0 and for $h(\boldsymbol{x}) \geq 0.5$ the algorithm will predict 1. \\

As we said, based on this intuition, we can interpret the hypothesis function as the probability of the input to be 1 hence:
\bse
P(y=1 | \boldsymbol{x}; \boldsymbol{w}) = h(\boldsymbol{x})
\ese

\vspace{5pt}

Of course since the output must be either 0 or 1 we get:
\begin{align*}
& P(y=0 | \boldsymbol{x}; \boldsymbol{w}) + P(y=1 | \boldsymbol{x}; \boldsymbol{w}) = 1 \Rightarrow \\
& P(y=0 | \boldsymbol{x}; \boldsymbol{w}) = 1 - P(y=1 | \boldsymbol{x}; \boldsymbol{w}) \Rightarrow \\
& P(y=0 | \boldsymbol{x}; \boldsymbol{w}) = 1 - h(\boldsymbol{x})
\end{align*}

We can combine these two probabilities in one in the following way:
\bse
P(y | \boldsymbol{x}; \boldsymbol{w}) = h(\boldsymbol{x})^y \cdot (1 - h(\boldsymbol{x}))^{(1-y)}
\ese

\vspace{5pt}

So in logistic regression the output follows a Bernoulli distribution with parameter $h(\boldsymbol{x})$. Now that we have a probability distribution, similarly to the linear regression, we can use the principle of the maximum likelihood in order to obtain the best parameters that maximize the likelihood. Thus, we will obtain the loss function for the logistic regression case. \\

By making again the assumption that we are dealing with independent and identically distributed random variables, for the likelihood it is
\bse
\mathcal{L} (\boldsymbol{w} |y ) = P(y^{(1)}, y^{(2)}, \ldots, y^{(m)} | \boldsymbol{x}^{(1)}, \boldsymbol{x}^{(2)}, \ldots, \boldsymbol{x}^{(m)} ; \boldsymbol{w}) = \prod_{i=1}^{m} P(y^{(i)} | \boldsymbol{x}^{(i)} ; \boldsymbol{w})
\ese

By substituting the probability:
\bse
\mathcal{L} (\boldsymbol{w} |y )  = \prod_{i=1}^{m} h(\boldsymbol{x}^{(i)})^{y^{(i)}} \cdot (1 - h(\boldsymbol{x}^{(i)}))^{(1 - {y^{(i)}})}
\ese

Subsequently for the log-likelihood:
{\setlength{\jot}{10pt}
\begin{align*}
l (\boldsymbol{w} |y ) &= \ln \mathcal{L} (\boldsymbol{w} |y ) \\ 
&= \ln \Big[ \prod_{i=1}^{m} h(\boldsymbol{x}^{(i)})^{y^{(i)}} \cdot (1 - h(\boldsymbol{x}^{(i)}))^{(1 - {y^{(i)}})} \Big] \\
&= \sum_{i=1}^{m} \ln \Big[ h(\boldsymbol{x}^{(i)})^{y^{(i)}} \cdot (1 - h(\boldsymbol{x}^{(i)}))^{(1 - {y^{(i)}})} \Big] \\
&= \sum_{i=1}^{m} \Big[ \ln \Big( h(\boldsymbol{x}^{(i)})^{y^{(i)}} \Big) + \ln \Big( (1 - h(\boldsymbol{x}^{(i)}))^{(1 - {y^{(i)}})} \Big) \Big] \\
&= \sum_{i=1}^{m} \Big[ y^{(i)} \cdot  \ln h(\boldsymbol{x}^{(i)})  + (1 - {y^{(i)}}) \cdot   \ln (1 - h(\boldsymbol{x}^{(i)})) \Big]
\end{align*}}

Once again, according to the principle of maximum likelihood, the best parameters can be found by maximizing the log-likelihood. Hence:
{\setlength{\jot}{10pt}
\begin{align*}
\boldsymbol{w} &= \argmax_{\boldsymbol{w}} [l (\boldsymbol{w} |y )] \\
&= \argmax_{\boldsymbol{w}} \Big[ \sum_{i=1}^{m} \Big( (y^{(i)} \cdot  \ln h(\boldsymbol{x}^{(i)})  + (1 - {y^{(i)}}) \cdot   \ln (1 - h(\boldsymbol{x}^{(i)})) \Big) \Big] \\
&= \argmin_{\boldsymbol{w}} \Big[ - \sum_{i=1}^{m} \Big( y^{(i)} \cdot  \ln h(\boldsymbol{x}^{(i)})  + (1 - {y^{(i)}}) \cdot  \ln (1 - h(\boldsymbol{x}^{(i)})) \Big) \Big]
\end{align*}}

At this point we can formally define the following function:

\bd [Cross Entropy Loss Function]
\textbf{Cross entropy loss function}  is defined as:
\bse
J(\boldsymbol{w}) = - \frac{1}{m} \sum_{i=1}^{m}  \Big( y^{(i)} \cdot  \ln h(\boldsymbol{x}^{(i)})  + (1 - {y^{(i)}}) \cdot   \ln (1 - h(\boldsymbol{x}^{(i)})) \Big)
\ese
\ed

The cross entropy is the loss function of the logistic regression in the similar way where MSE loss function is the loss function for linear regression. The name \say{cross entropy} comes from the definition of cross entropy which is the average amount of information needed to identify an event  between two probability distributions $p$ and $q$ over the same underlying set of events \\

Notice that the only possible values of $y^{(i)}$ is 0 or 1. This means that in any case, one of the terms $y^{(i)}$ or $(1-y^{(i)})$ in $J$ will vanish and the other one will be equal to 1. So in the end the only thing that is actually part of the loss is the logarithm of the hypothesis function, which given that the hypothesis function is a sigmoid function which is always between 0 and 1, the logarithm is always negative. With the overall negative sign the loss turns positive and this is what we want to minimize.\\

We are not gonna write a vectorized form for the cross entropy loss function for two reasons. First of all, not all matrices have a logarithm and those matrices that do have a logarithm may have more than one logarithm. So one has to be careful when uses the vectorized form of logistic regression because it carries logarithms of matrices. Secondly, derivatives of logarithms of non square matrices sometimes are not defined.  Since we need to calculate the derivative of $J$ we might get problems. For this reason we will use the non vectorized form for the calculations, however we will express the final result in a vectorized form.\\

As in the linear case, the principle of maximum likelihood leads to the minimization of the cross entropy loss function in order to obtain the best parameters. We will examine the same techniques that we developed for the gradient descent in the linear case, i.e normal equation and gradient descent.

\subsection{Normal Equation}

As we already mentioned, since we want to minimize a function the straight forward way of doing that is to calculate the derivative and then set it to 0. However, in the case of logistic regression the normal equation does not apply since there is no closed analytical solution of $\nabla_{\boldsymbol{w}} J(\boldsymbol{w})=0$. The only way for solving the optimization problem is through gradient descent.

\subsection{Gradient Descent}

Gradient descent  works fine in logistic regression. First, let's calculate the derivative of $J(\boldsymbol{w})$ for logistic regression.

{\setlength{\jot}{15pt}
\begin{align*}
\nabla_{\boldsymbol{w}} J(\boldsymbol{w}) &= - \frac{1}{m}  \nabla_{\boldsymbol{w}} \Big[\sum_{i=1}^{m}  \Big( y^{(i)} \cdot  \ln h(\boldsymbol{x}^{(i)})  + (1 - {y^{(i)}}) \cdot   \ln (1 - h(\boldsymbol{x}^{(i)})) \Big) \Big] \\
&= - \frac{1}{m} \sum_{i=1}^{m}  \Big( y^{(i)} \cdot  \nabla_{\boldsymbol{w}} \ln h(\boldsymbol{x}^{(i)})  + (1 - {y^{(i)}}) \cdot \nabla_{\boldsymbol{w}} \ln (1 - h(\boldsymbol{x}^{(i)})) \Big)  \\
&= - \frac{1}{m} \sum_{i=1}^{m}  \Big( y^{(i)} \cdot  \frac{\nabla_{\boldsymbol{w}} h(\boldsymbol{x}^{(i)})}{h(\boldsymbol{x}^{(i)})}   - (1 - {y^{(i)}}) \cdot \frac{\nabla_{\boldsymbol{w}}  h(\boldsymbol{x}^{(i)})}{1 - h(\boldsymbol{x}^{(i)})}  \Big)\\
&= - \frac{1}{m} \sum_{i=1}^{m}  \Big( \frac{y^{(i)}}{h(\boldsymbol{x}^{(i)})}  -  \frac{1 - {y^{(i)}}}{1 - h(\boldsymbol{x}^{(i)})}  \Big) \cdot \nabla_{\boldsymbol{w}} h(\boldsymbol{x}^{(i)})  \\
&= - \frac{1}{m} \sum_{i=1}^{m}  \Big( \frac{y^{(i)} \cdot (1 - h(\boldsymbol{x}^{(i)}) - (1 - {y^{(i)}}) \cdot h(\boldsymbol{x}^{(i)})}{h(\boldsymbol{x}^{(i)}) \cdot (1 - h(\boldsymbol{x}^{(i)}))} \Big) \cdot  \nabla_{\boldsymbol{w}} \Big[  \frac{1}{1 + exp(- \boldsymbol{w}^T \boldsymbol{x}^{(i)})} \Big] \\
&= - \frac{1}{m} \sum_{i=1}^{m}  \Big( \frac{y^{(i)} - y^{(i)} \cdot h(\boldsymbol{x}^{(i)}) - h(\boldsymbol{x}^{(i)})  + {y^{(i)}} \cdot h(\boldsymbol{x}^{(i)})}{h(\boldsymbol{x}^{(i)}) \cdot (1 - h(\boldsymbol{x}^{(i)}))} \Big) \cdot \Big( \frac{-1}{(1 + exp(- \boldsymbol{w}^T \boldsymbol{x}^{(i)}))^2} \cdot exp(- \boldsymbol{w}^T \boldsymbol{x}^{(i)}) \cdot (-\boldsymbol{x}^{(i)})  \Big) \\
&= - \frac{1}{m} \sum_{i=1}^{m}  \Big( \frac{y^{(i)} - h(\boldsymbol{x}^{(i)})}{h(\boldsymbol{x}^{(i)}) \cdot (1 - h(\boldsymbol{x}^{(i)}))} \Big) \cdot \Big( h(\boldsymbol{x}^{(i)})^2 \cdot \frac{1 - h(\boldsymbol{x}^{(i)})}{h(\boldsymbol{x}^{(i)})} \cdot \boldsymbol{x}^{(i)} \Big) \\
&= \frac{1}{m} \sum_{i=1}^{m} \frac{h(\boldsymbol{x}^{(i)}) - y^{(i)})}{h(\boldsymbol{x}^{(i)}) \cdot (1 - h(\boldsymbol{x}^{(i)}))} \cdot h(\boldsymbol{x}^{(i)}) \cdot (1 - h(\boldsymbol{x}^{(i)}))\cdot \boldsymbol{x}^{(i)} \\
&= \frac{1}{m} \sum_{i=1}^{m} (h(\boldsymbol{x}^{(i)}) - y^{(i)}) \cdot \boldsymbol{x}^{(i)}
\end{align*}}

Hence the update rule reads:
\bse
\boldsymbol{w} \coloneqq \boldsymbol{w} - \frac{\alpha}{m} \sum_{i=1}^{m} \Big( \frac{1}{1 + exp(- \boldsymbol{w}^T \boldsymbol{x}^{(i)})} - y^{(i)} \Big) \cdot \boldsymbol{x}^{(i)}
\ese

\vspace{5pt}

Or in vectorized form:
\bse
\boldsymbol{w} \coloneqq \boldsymbol{w} - \frac{\alpha}{m} X^T \Big( \frac{1}{1+exp(-X \boldsymbol{w})}-\boldsymbol{y} \Big)
\ese

\vspace{5pt}

At this point, notice that gradient descent can be generalized into one model for both linear and logistic regression since the update rule for both of them can be be written in one coherent way as:
\bse
\boldsymbol{w} \coloneqq \boldsymbol{w} - \frac{\alpha}{m} X^T \Big( h(X)-\boldsymbol{y} \Big)
\ese

\vspace{5pt}

where one has to use either MSE loss function or cross entropy loss function depending on the regression problem!

\section{Generalized Linear Model}

As we showed, in linear regression the target follows a normal distribution while in logistic regression the target follows a Bernoulli distribution. We can generalize both regressions in one coherent model called \say{generalized linear model} in which the target is allowed to follow a broad family of probability distributions.

\bd[Generalized Linear Model]
\textbf{Generalized linear model} (GLM) is a model that allows the dependent variable to follow an exponential family of probability distributions of the form:
\bse
P(y | \eta) = b(y) \cdot exp(\eta^T T(y) - a(\eta))
\ese
\ed

By picking specific values for $b$, $\eta$, $T$ and $a$ we end up with different distributions (including linear and logistic regression). Then we simply assume independence and apply the principle of maximum likelihood to obtain a loss function, in order to minimize it and find the best parameters. 

\section{Errors}

Since $h$ is an estimator of $f$, the theory we developed in the chapter of parametric inference for estimators also holds for $h$. In other words, for the hypothesis function, which acts as an estimator for $f$, we can define quantities such as MSE, sampling deviation, bias and variance.\\

Out of all possible quantities that can be defined, MSE, bias and variance are of crucial importance in machine learning, since we use them in order to evaluate how well a mahcine learning model performs. Let us see now the definitions of these quantities adjusted for the case of machine learning where the estimator is $h$.

\subsection{Point-Wise, Overall, In-Sample \& Out-Of-Sample Error}

Starting from the corresponding MSE in machine learning case we define the following quantities:

\bd[Point-Wise Error]
We define the \textbf{point-wise error} $e$ as a function of the real target function $f$ and the hypothesis function $h$ at point $x$:
\bse
e = e( f(x), h(x) )
\ese
\ed

For example, for linear regression we could use $e( f(x), h(x) ) = (f(x) - h(x))^2$ while for logistic regression $e( f(x), h(x) ) = [f(x) \neq h(x)]$. Given point-wise error we can generalize to overall error.
\bd[Overall Error]
We define the \textbf{overall error} $E$ as the average over all point-wise errors $e( f(x), h(x) )$ at every point $x$. 
\ed

We distinguish between two kind of overall errors: the in-sample error and the out-of-sample error.

\bd[In-Sample Error]
We define the \textbf{in-sample error} $E_{in}$ as  the average of point-wise errors of the dataset that the model was trained:
\bse
E_{in} = \frac{1}{m} \sum_{i=1}^m e( f(x^{(i)}), h(x^{(i)}) )
\ese
\ed

\vspace{5pt}

In other words in-sample error shows how well the model performs on the data used to build it.

\bd[Out-Of-Sample Error]
We define the \textbf{out-of-sample error} $E_{out}$ as a the expected value of point-wise errors of new data:
\bse
E_{out} = E_{x}[e( f(x), h(x) )]
\ese
\ed

\vspace{5pt}

In other word out-of-sample error show how well the model generalizes to predictions for data it has not seen before. It makes sense that in order for $h$ to work well out of sample, so it can predict, it must be $E_{out} \approx 0$.

\subsection{Bias \& Variance}

Back in parametric inference chapter, we introduced the bias $B$ of an estimator $\hat{\theta}$,  as the difference between the expected value of the estimator and the actual true parameter we want to estimate, $B = E[\hat{\theta}_{n}] - \theta$. Coming to our case where our estimator is $\hat{\theta} = h(x)$ and the true parameter is the target funtion  $\theta = f(x)$, for the bias we get:

\bd[Bias]
We define the \textbf{bias} $B$ of the hypothesis function $h$ as the quantity:
\bse
B = E[h(x)] - f(x)
\ese
\ed

The bias error is an error from erroneous assumptions in the learning algorithm. When we are dealing with high bias, formally we can say that the hypothesis set $H=\{h\}$ was not big enough in order to contain function that can approximate well the target function $f$. So our best approximation for $f$ is still a bad one that cannot fit the data well.

\vspace{5pt}

\begin{figure}[H]
\includegraphics[scale=0.45]{images/bias.png}
\centering
\end{figure}

\vspace{5pt}

High bias can cause an algorithm to miss the relevant relations between features and target outputs and fail to capture the underlying structure of the dataset. We call this a case of underfitting, since the model fails to fit the given dataset well.

\vspace{5pt}

\begin{figure}[H]
\includegraphics[scale=0.9]{underfitting.png}
\centering
\end{figure}

\vspace{5pt}

Underfitting is one of the two main problems that a machine learning model can have and it leads to a high in-sample error $E_{in}$ which subsequently leads to a high out-of-sample error $E_{out}$. Hence even that the problem is coming from the in-sample error it leads to not being able to generalize for out-of-sample data.\\

In parametric inference chapter, we also defined  the variance of an estimator $\hat{\theta}_{n}$ as the expected value of the square difference of the estimator from the expected value of the estimator: $Var = E[(\hat{\theta}_{n} - E[\hat{\theta}_{n}])^2]$.  Coming back to machine learning for the variance we get:

\bd[Variance]
We define the \textbf{variance} $B$ of the hypothesis function $h$ as the quantity:
\bse
Var = E_{x}[(h(x) - E_{x}[h(x)])^2]
\ese
\ed

The variance is an error from sensitivity to small fluctuations in the training set.

\vspace{5pt}

\begin{figure}[H]
\includegraphics[scale=0.45]{variance.png}
\centering
\end{figure}

\vspace{5pt} 

When we are dealing with high variance, informally we can say that the hypothesis set $H=\{h\}$ is very big so in order to compensate the spread of dataset the model finds a function that fits the particular data very well but fails to generalize to new data. We call this a case of overfitting, since the model fails to generalize to new data.

\vspace{5pt}

\begin{figure}[H]
\includegraphics[scale=0.9]{overfitting.png}
\centering
\end{figure}

\vspace{5pt}

Overfitting leads to a very low in-sample $E_{in} \approx 0$, since it does a very good job on fitting the given data. However it fails to generalize, hence to predict new data, which leads to a very high out-of-sample error $E_{out}$.\\

Back in parametric inference we also showed that the mean squared error can be decomposed to bias and variance, and of course the same holds in our case since for the out of sample error of linear regression we can show:

{\setlength{\jot}{10pt}
\begin{align*}
E_{out} &= E_{x} \Big[ e(f(x),h(x)) \Big]  \\
&=  E_{x} \Big[ \Big( f(x) - h(x) \Big)^2 \Big] \\
&= E_{x} \Big [ \Big( f(x) - h(x) + E_{x}[h(x)] - E_{x}[h(x)] \Big)^2 \Big] \\
&= E_{x} \Big[ \Big( \Big( f(x) - E_{x}[h(x)] \Big) + \Big( E_{x}[h(x)] - h(x) \Big) \Big)^2 \Big] \\
&= E_{x} \Big[ \Big( f(x) - E_{x}[h(x)] \Big)^{2} + 2 \Big( f(x) - E_{x}[h(x)] \Big) \Big( E_{x}[h(x)] - h(x) \Big) + \Big( E_{x}[h(x)] - h(x) \Big)^2 \Big] \\
&= E_{x} \Big[ \Big( f(x) - E_{x}[h(x)] \Big)^{2} \Big] + E_{x} \Big[ 2 \Big( f(x) - E_{x}[h(x)] \Big) \Big( E_{x}[h(x)] - h(x) \Big) \Big] + E_{x} \Big[ \Big( E_{x}[h(x)] - h(x) \Big)^{2} \Big] \\
&= \Big( f(x) - E_{x}[h(x)] \Big)^{2}  + 2 \Big( f(x) - E_{x}[h(x)] \Big)  \Big( E_{x} \Big[ E_{x}[h(x)] - h(x)  \Big] \Big) + E_{x} \Big[ \Big( E_{x}[h(x)] - h(x) \Big)^{2} \Big] \\
&= \Big( f(x) - E_{x}[h(x)] \Big)^{2}  + 2 \Big( f(x) - E_{x}[h(x)] \Big)  \Big(  E_{x}[h(x)] - E_{x}[h(x)] \Big) + E_{x} \Big[ \Big( E_{x}[h(x)] - h(x) \Big)^{2} \Big] \\
&= \Big( f(x) - E_{x}[h(x)] \Big)^{2} + E_{x} \Big[ \Big( E_{x}[h(x)] - h(x) \Big)^{2} \Big] \\
&= B^2 + Var
\end{align*}}

Hence, the out-of-sample error is actually a combination of bias and variance. So in order to have a model that generalizes well, we have to keep both of them low. However, since they are of opposite nature, the more we reduce bias the more variance increases and vice versa. This is the so called \say{bias-variance trade off}. The goal in machine learning is to balance this trade off so the model fits the data well and doesn't fail to generalize.

\begin{figure}[H]
\includegraphics[scale=0.7]{overunderfitting.png}
\centering
\end{figure}

\vspace{5pt}

In this graph we see that in the case of high bias (underfitting) we have restricted our model to linear predictors, however the data do not follow a linear trend, hence the hypothesis set is too small and the model cannot find a good curve to fit the data. On the other hand, in the case of high variance (overfitting) the hypothesis set is so big allowing complex predictors so the model managed to find a high degree polynomial that fit the data really good, however it will fail to generalize since it depends a lot on the initial values of the data and it is sensitive to fluctuations of them. Finally in the last graph we have a good balance of bias and variance and the model found a good curve!

\section{Evaluation}

Evaluation is about how good a model generalizes to new data. After applying the learning algorithm to the data and having obtained a hypothesis $h$, the machine learning model is ready to make new predictions. However before that, we have to evaluate the model by analysing the errors that we just introduced. \\

The starting part is the in-sample and out-of-sample errors that we defined previously as:
\bse
E_{in} = \frac{1}{m} \sum_{i=1}^m e( f(x^{(i)}), h(x^{(i)}) ) \qquad \text{and}  \qquad E_{out} = E_{x}[e( f(x), h(x) )]
\ese

\vspace{5pt}

In general, for the error function $e$ we use the corresponding loss function $J$ that we used to train the model, since it is a function of the target and hypothesis functions as $e$, and it is a really good measure of error:
\bse
E_{in} = \frac{1}{m} \sum_{i=1}^m J^{{(i)}}( f(x^{(i)}), h(x^{(i)}) ) \qquad \text{and}  \qquad E_{out} = E_{x}[J( f(x), h(x) )]
\ese

\vspace{5pt}

where here the notation $J^{{(i)}}$ means the error coming from the i'th training example. Hence, now $E_{in}$ is calculated with the data that we trained the model, so it's a very good measure of how well the model performs in the data that it was trained on. The problem comes from $E_{out}$ since we don't know how to compute this expected value. Unsurprising we will perform the usual trick of substituting the expected value with the average so:
\bse
E_{in} = \frac{1}{m} \sum_{i=1}^m J^{{(i)}}( f(x^{(i)}), h(x^{(i)}) )  \qquad \text{and}  \qquad E_{out} = \frac{1}{m} \sum_{i=1}^m J^{{(i)}}( f(x^{(i)}), h(x^{(i)}) )
\ese

\vspace{5pt}

Of course since  we estimate the expected value with an average that brings an error to the estimation of the out-of-sample error. However for our purposes we assume that this error is neglectful, and from now on we will treat the estimated out-of-sample error as the actual out-of-sample error. In general we have to keep in mind though that out-of-sample error carries an error.\\

The question that arises is what data are we going to use for $E_{out}$. Using the same data that we trained the model is a really bad idea since, first of all, we will simply get $E_{out} = E_{in}$ and secondly the model already knows the correct answers of the data since we used them to train it, and the evaluation will be biased.\\

In order to overcome this problem, we split the dataset (before training the model) into two parts: training set and evaluation set. Then we use the first to train the model and obtain $E_{in}$  and the latter to evaluate its performance and obtain $E_{out}$. Since the model is trained with the train set, it has never seen the evaluation set so the estimation of the out-of-sample error with the evaluation set will be unbiased.\\

One of the things to consider is the proportions of splitting the dataset intro training set and evaluation set. This again is an area of heavy research, but in general in machine learning we usually split them either in a proportion of \say{70\% - 30\%} or \say{80\% - 20\%} depending on the amount of data. (In all cases the largest proportion goes for training the model). In  other areas of machine learning like deep learning where we usually have a very large amount of data we use splitting rules of \say{99\% - 1\%}. But we will address this issue in details in deep learning chapter. \\

Hence, before training we split the dataset as:
\bit
\item Training Dataset: $\{x_{\text{train}}^{(i)}, y_{\text{train}}^{(i)}\}, \:\:\: i = 1,2,\ldots,m_{\text{train}} \qquad$ (70\% or 80\% of initial dataset)   \\ 
\item Test Dataset: $\{x_{\text{eval}}^{(i)}, y_{\text{eval}}^{(i)}\}, \:\:\: i = 1,2,\ldots,m_{\text{eval}} \qquad$ (30\% or 20\% of initial dataset) \\
\eit

And subsequently the errors become:
\bse
E_{in} = \frac{1}{m_{\text{train}}} \sum_{i=1}^{m_{\text{train}}} J^{{(i)}}( f(x^{(i)}), h(x^{(i)}) )  \qquad \text{and}  \qquad E_{out} = \frac{1}{m_{\text{eval}}} \sum_{i=1}^{m_{\text{eval}}} J^{{(i)}}( f(x^{(i)}), h(x^{(i)}) )
\ese

\vspace{5pt}

From now on we will be referring to the first expression as $J_{\text{train}} = E_{in}$ and to the second one as $J_{\text{eval}} = E_{out}$.\\

So for example for linear regression we would have:
\bse
J_{\text{train}} = \frac{1}{2m_{train}} \sum_{i=1}^{m_{train}} (y^{(i)} - \boldsymbol{w}^T \boldsymbol{x}^{(i)})^2 \qquad \text{and}  \qquad J_{\text{eval}} = \frac{1}{2m_{eval}} \sum_{i=1}^{m_{eval}} (y^{(i)} - \boldsymbol{w}^T \boldsymbol{x}^{(i)})^2
\ese

\vspace{5pt}

While for logistic regression we would have
\bse
J_{\text{train}}  = - \frac{1}{m_{train}} \sum_{i=1}^{m_{train}}  \Big( y^{(i)} \cdot  \ln h(\boldsymbol{x}^{(i)})  + (1 - {y^{(i)}}) \cdot   \ln (1 - h(\boldsymbol{x}^{(i)})) \Big)
\ese

\vspace{5pt}

and
\bse
J_{\text{eval}}  = - \frac{1}{m_{eval}} \sum_{i=1}^{m_{eval}}  \Big( y^{(i)} \cdot  \ln h(\boldsymbol{x}^{(i)})  + (1 - {y^{(i)}}) \cdot   \ln (1 - h(\boldsymbol{x}^{(i)})) \Big)
\ese

\vspace{5pt}

Now that we have $J_{\text{train}} $ and $J_{\text{eval}} $ we know how well the models performs in and out of sample. However we can also use them in order to find if the model suffers from underfitting or overfitting. There are two ways that we can do so.\\

The first way, is by gradually increasing the complexity of the model (higher polynomial degrees so bigger hypothesis set), training the model for each complexity level and calculate both $J_{train}$ and $J_{eval}$ for each model. Then by plotting out the different values for different levels of complexity we usually end up with the following graph:

\begin{figure}[H]
\includegraphics[scale=0.35]{eval.png}
\centering
\end{figure}

In the high bias area both $J_{train}$ and $J_{eval}$ are high. This means that the model does not fit the training data well hence it fails to generalize. This is the case of underfitting. In the high variance area, $J_{train}$ is low but $J_{eval}$ is high. This means that the model fits the training data well but fails to generalize to unseen data. This is the case of overfitting. Hence by using the graph we can diagnose both cases!\\

The second way to diagnose the problem of the model, is through the so called \say{learning curves}.

\bd[Learning Curve]
A \textbf{learning curve} is a graphical representation of how an increase in learning comes from greater experience; or how the more someone performs a task, the better they get at it. 
\ed

Informally, a learning curve is the relation between error (as expressed in loss function) and training examples $m$. By plotting this relation for both  $J_{train}$ and $J_{eval}$  we end up with  two learning curves and by their relative position we can diagnose if our model suffers from high bias or high variance.\\

More specifically, when the learning curves of $J_{train}$ and $J_{eval}$ do not have a large gap between them as $m$ increases we are usually dealing we a case of high bias and underfitting. 

\vspace{10pt}

\begin{figure}[H]
\includegraphics[scale=0.5]{lchighbias.png}
\centering
\end{figure}

\vspace{10pt}

On the other hand when there is a large gap between the two curves we are dealing with the case of high variance and overfitting.

\vspace{10pt}

\begin{figure}[H]
\includegraphics[scale=0.53]{lchighvariance.png}
\centering
\end{figure}

\vspace{10pt}

Once we detect the problem then we have to make some changes in order to fix them! Here are some of the techniques that we follow:
\bit
\item For high bias (underfitting):
\bit
\item Increase model complexity (polynomial terms)
\item Increase number of features
\eit
\item For high variance (overfitting):
\bit
\item Decrease model complexity (polynomial terms)
\item Decrease number of features
\item Find more training examples
\item Regularization (next section)
\eit
\eit

\section{Regularization}

As we saw in the previous section, when we allow a very broad hypothesis set with many higher order terms the model might find a hypothesis function $h$ that gives a 0 in-sample error but fails to generalize. This is due to high variance, i.e large dependence on the very specific dataset used for training, and we call this a case of overfitting. A way to deal with overfitting is a collection of techniques that undergo with the name \say{regularization}.

\vspace{10pt}

\begin{figure}[H]
\includegraphics[scale=0.07]{regularization.png}
\centering
\end{figure}

\bd[Regularization]
Regularization is the process of adding information in order to solve an ill-posed problem and to prevent overfitting.
\ed

We will see many different regularization techniques throughout the notes. For now we will start with  \say{Ridge Regression} or \say{L2 Regularization} and \say{Lasso Regression } or  \say{L1 Regularization}.

\subsection{Ridge Regression - L2 Regularization}

The reason of overfitting is that the parameters $\boldsymbol{w}$ are free to get any value. With regularization we penalize the parameters by imposing an extra constraint on $\boldsymbol{w}$ of the form:
\bse
\boldsymbol{w}^{T} \boldsymbol{w} \leq C
\ese

where $C$ is a constant defined by us and it controls the effect of regularization. It is called \say{L2 regularization} because the quantity $\boldsymbol{w}^{T} \boldsymbol{w}$ is actually the squared L2 norm of the vector $\boldsymbol{w}$:
\bse
||\boldsymbol{w}||^2_2 = \boldsymbol{w}^{T} \boldsymbol{w}
\ese

Hence now the optimization problem becomes to minimize the loss function $J(\boldsymbol{w})$ subject to the above mentioned constraint. According to (Appendix \ref{Constrained Optimization}) in order to do so we define the Lagrangian:
\bse
\mathcal{L} (\boldsymbol{w}) = J(\boldsymbol{w}) + \frac{\lambda}{2m} \boldsymbol{w}^{T} \boldsymbol{w}
\ese

\vspace{5pt}

where $\lambda$ is the Lagrange multiplier, and then we solve the equation:
\bse
\nabla_{\boldsymbol{w}} \mathcal{L} (\boldsymbol{w}) = 0
\ese

For example, for linear regression where $J(\boldsymbol{w})$ is given by (\ref{eq:lrloss}) the Lagrangian reads:
{\setlength{\jot}{10pt}
\begin{align*} 
\mathcal{L} (\boldsymbol{w}) &= J(\boldsymbol{w}) + \frac{\lambda}{2m} \boldsymbol{w}^{T} \boldsymbol{w} =  \frac{1}{2m} (X \boldsymbol{w} - \boldsymbol{y})^T (X \boldsymbol{w} - \boldsymbol{y}) + \frac{\lambda}{2m} \boldsymbol{w}^{T} \boldsymbol{w} 
\end{align*}}

At this point we can redefine this Lagrangian as a new loss function of the form:
\bse
J (\boldsymbol{w}) =  \frac{1}{2m} \Big[ (X \boldsymbol{w} - \boldsymbol{y})^T (X \boldsymbol{w} - \boldsymbol{y}) + \lambda \boldsymbol{w}^{T} \boldsymbol{w} \Big ]
\ese

\vspace{5pt}

and then the problem is to minimize this loss function which is actually a regression problem. The corresponding regression is called \say{Ridge regression}, where the only difference with linear regression is that we have to add the extra term in the loss function to reduce overfitting.\\

The coefficient $\lambda$ is the one that controls the regularization effect on the regression. In one extreme where $\lambda=0$ the regularization term vanishes, and the loss function ends up to the mean squared error loss function, hence the ridge regression turns to linear regression. In the other extreme where $\lambda \to \infty$ then the regularization term penaltizes all parameters in an extreme way, so the ridge regression, in order to minimize the loss, is forced to set all the parameters to 0. In the end we end up with $\boldsymbol{w}^{T} \boldsymbol{x} = 0$. For all intermediate values of $\lambda$ we get different levels of regularization. It is actually our job to tune the model to the right $\lambda$ that does the job.\\

Now that we have a loss function, we treat the problem in the similar way as we did before. For example, in the linear case of ridge regression we can solve the normal equation in the same way we solved it before:
{\setlength{\jot}{10pt}
\begin{align*}
J(\boldsymbol{w}) &= \frac{1}{2m} \Big[ (X \boldsymbol{w} - \boldsymbol{y})^T (X \boldsymbol{w} - \boldsymbol{y}) + \lambda \boldsymbol{w}^{T} \boldsymbol{w} \Big]\\
&= \frac{1}{2m} \Big[ \Big( (X \boldsymbol{w})^T - \boldsymbol{y}^T \Big) \Big( X \boldsymbol{w} - \boldsymbol{y} \Big) + \lambda \boldsymbol{w}^{T} \boldsymbol{w} \Big]\\
&= \frac{1}{2m}  \Big[ (X \boldsymbol{w})^T (X \boldsymbol{w}) - (X \boldsymbol{w})^T \boldsymbol{y} - \boldsymbol{y}^T (X \boldsymbol{w}) + \boldsymbol{y}^T \boldsymbol{y} + \lambda \boldsymbol{w}^{T} \boldsymbol{w} \Big] \\
&= \frac{1}{2m}  \Big[ (X \boldsymbol{w})^T (X \boldsymbol{w}) - 2 (X \boldsymbol{w})^T \boldsymbol{y} + \boldsymbol{y}^T \boldsymbol{y} + \lambda \boldsymbol{w}^{T} \boldsymbol{w} \Big] \\
&= \frac{1}{2m}  \Big[ \boldsymbol{w}^T X^T X \boldsymbol{w} - 2 \boldsymbol{w}^T X^T \boldsymbol{y} + \boldsymbol{y}^T \boldsymbol{y} + \lambda \boldsymbol{w}^{T} \boldsymbol{w} \Big]
\end{align*}}

By setting the derivative to 0 we obtain:
{\setlength{\jot}{10pt}
\begin{align*}
& \nabla_{\boldsymbol{w}} J(\boldsymbol{w}) = 0 \Rightarrow \\
& \frac{1}{2m} \nabla_{\boldsymbol{w}}  \Big[ \boldsymbol{w}^T X^T X \boldsymbol{w} - 2 \boldsymbol{w}^T X^T \boldsymbol{y} + \boldsymbol{y}^T \boldsymbol{y} + \lambda \boldsymbol{w}^{T} \boldsymbol{w} \Big] \\
& \frac{1}{2m} \Big[ 2  X^T X \boldsymbol{w} - 2 X^T \boldsymbol{y} + 2\lambda \boldsymbol{w} \Big] = 0 \Rightarrow \\
& \frac{1}{m} \Big[ X^T X \boldsymbol{w} - X^T \boldsymbol{y} + \lambda \boldsymbol{w}  \Big] = 0 \Rightarrow \\
& X^T X \boldsymbol{w} - X^T \boldsymbol{y} + \lambda \boldsymbol{w} = 0 \Rightarrow \\
& (X^T X + \lambda I) \boldsymbol{w} = X^T \boldsymbol{y} \Rightarrow \\
& \underbrace{(X^T X + \lambda I)^{-1} (X^T X + \lambda I)}_{I} \boldsymbol{w} = (X^T X + \lambda I)^{-1} X^T \boldsymbol{y} \Rightarrow \\
& \boldsymbol{w} = (X^T X + \lambda I)^{-1} X^T \boldsymbol{y}
\end{align*}}

The only difference with the normal equation of linear regression is the extra term $\lambda I$ coming from regularization. \\

Gradient descent also works for ridge regression. For the derivative of $J$:
\bse
\nabla_{\boldsymbol{w}} J(\boldsymbol{w}) = \frac{1}{m} \Big(X^T X \boldsymbol{w} - X^T \boldsymbol{y} + \lambda \boldsymbol{w} \Big) = \frac{1}{m} X^T \Big( (X + \lambda I) \boldsymbol{w} -\boldsymbol{y} \Big)
\ese

\vspace{5pt}

Hence the update rule reads:
\bse
\boldsymbol{w} \coloneqq \boldsymbol{w} - \frac{\alpha}{m} X^T \Big( (X + \lambda I) \boldsymbol{w} -\boldsymbol{y} \Big)
\ese

\vspace{5pt}

Of course, L2 regularization can be applied also for the case of logistic regression. More specifically, for cross entropy loss function of logistic regression $J(\boldsymbol{w})$ the Lagrangian reads:
{\setlength{\jot}{10pt}
\bse
\mathcal{L} (\boldsymbol{w}) = J(\boldsymbol{w}) + \frac{\lambda}{2m} \boldsymbol{w}^{T} \boldsymbol{w} =  - \frac{1}{m} \Big( \boldsymbol{y}^T \cdot  \ln h(X)  + (I - \boldsymbol{y})^T \cdot   \ln (I - h(X)) \Big) + \frac{\lambda}{2m} \boldsymbol{w}^{T} \boldsymbol{w}
\ese

\vspace{5pt}

Similarly to the linear case, we redefine this Lagrangian as a new loss function of the form:
\bse
J (\boldsymbol{w}) =  - \frac{1}{m} \Big[ \boldsymbol{y}^T \cdot  \ln h(X)  + (I - \boldsymbol{y})^T \cdot   \ln (I - h(X)) - \frac{\lambda}{2} \boldsymbol{w}^{T} \boldsymbol{w} \Big ]
\ese

\vspace{5pt}

As we said back in logistic regression, normal equation does not apply here since there is no closed analytical solution, however gradient descent still applies where the rule simply reads:
\bse
\boldsymbol{w} \coloneqq \Big( 1 -  \frac{\alpha \lambda}{m} \Big)\boldsymbol{w} - \frac{\alpha}{m} X^T \Big( \frac{1}{1+exp(-X \boldsymbol{w})}-\boldsymbol{y} \Big)
\ese

\vspace{5pt}

In both cases solving ridge regression will give us as a result a solution that slightly underfits the data, compared to linear or logistic regression. This underfitting will produce higher bias hence, due to bias-variance trade off, a reduced variance which will lead to the reduction of overfitting.

\subsection{Lasso Regression - L1 Regularization}

In ridge regression, or L2 regression, we used the L2 norm of the vector $\boldsymbol{w}$.  Another way of regularization is to use L1 norm which is:
\bse
|| \boldsymbol{w} ||_1 \leq C
\ese

By repeating the same way of analysis as in ridge regression, we can define the following loss function for linear regression:
\bse
J (\boldsymbol{w}) =  \frac{1}{2m} \Big[ (X \boldsymbol{w} - \boldsymbol{y})^T (X \boldsymbol{w} - \boldsymbol{y}) + \lambda ||\boldsymbol{w}||_1 \Big ]
\ese

\vspace{5pt}

and for logistic regression:

\bse
J (\boldsymbol{w}) =  - \frac{1}{m} \Big[ \boldsymbol{y}^T \cdot  \ln h(X)  + (I - \boldsymbol{y})^T \cdot   \ln (I - h(X)) - \frac{\lambda}{2} ||\boldsymbol{w}||_1 \Big ]
\ese

\vspace{5pt}

The corresponding regression is called \say{Lasso regression}. As before, we can use normal equation and gradient descent to solve Lasso regression.

\section{Classification Error Metrics}

In classification problems where both input and output can be either 0 or 1, we can follow a different approach of error evaluation based on exact matches and mismatches between prediction and actual result. The usual case, since we are dealing with a binary output, is to define either 0 or 1 as the positive class and the remaining as the negative one. Which one is which depends on the nature of the problem. For now we will stick with the case were 0 represents the negative class and 1 the positive one.\\

Given that both the actual class and the predicted class can be either positive or negative we end up with 4 different, distinct situations. Let us define them formally:

\bd[True Positive]
\textbf{True positive} (TP) also called hit, is the case where the model predicts a positive result when the actual outcome is indeed positive.
\ed

\bd[True Negative]
\textbf{True negative} (TN) also called correct rejection, is the case where the model predicts a negative result when the actual outcome is indeed negative.
\ed

\bd[False Positive]
\textbf{False positive} (FP) also called false alarm or type I error, is the case where the model predicts a positive result when the actual outcome is negative.
\ed

\bd[False Negative]
\textbf{False negative} (FN) also called miss or type II error, is the case where the model predicts a negative result when the actual outcome is positive.
\ed

Once the model is trained, we test it on the evaluation set and we measure the number of occurrences of each category. Then we gather them all together to the so called \say{confusion matrix}.

\bd[Confusion Matrix]
\textbf{Confusion matrix} is a table that reports the number of true positives TP, true negatives TN, false positives FP and false negatives FN of a model.
\ed

\vspace{10pt}

\begin{figure}[H]
\includegraphics[scale=0.3]{confusion}
\centering
\end{figure}

\vspace{10pt}

Once we have constructed the confusion matrix we can define the following error metrics:

\bd[Accuracy]
\textbf{Accuracy} (ACC) is the rate that shows overall how often the model was correct:
\bse
ACC = \frac{TP + TN}{TP + TN + FP + FN}
\ese
\ed

\vspace{5pt}

\bd[Error Rate]
\textbf{Error rate} (ERR) also called misclassification, is the rate that shows overall how often the model was incorrect:
\bse
ERR = \frac{FP + FN}{TP + TN + FP + FN}
\ese
\ed

\vspace{5pt}

It is of course: $ACC + ERR = 1$

\vspace{5pt}

\bd[True Positive Rate]
\textbf{True positive rate} (TPR) also called sensitivity, recall or hit rate, is the rate that shows how often the model predicts positive when the actual outcome is indeed positive:
\bse
TPR = \frac{TP}{TP + FN}
\ese
\ed

\vspace{5pt}

\bd[False Negative Rate]
\textbf{False negative rate} (FNR) also called miss rate, is the rate that shows how often the model predicts negative when the actual outcome is positive:
\bse
FNR = \frac{FN}{TP + FN}
\ese
\ed

\vspace{5pt}

It is of course: $TPR + FNR = 1$

\vspace{5pt}

\bd[True Negative Rate]
\textbf{True negative rate} (TNR) also called specificity or selectivity, is the rate that shows how often the model predicts negative when the actual outcome is indeed negative:
\bse
TNR = \frac{TN}{TN + FP}
\ese
\ed

\vspace{5pt}

\bd[False Positive Rate]
\textbf{False positive rate} (FPR) also called fall-out rate, is the rate that shows how often the model predicts positive when the actual outcome is negative:
\bse
FPR = \frac{FP}{TN + FP}
\ese
\ed

\vspace{5pt}

It is of course: $TNR + FPR = 1$

\vspace{5pt}

\bd[Positive Predicted Value]
\textbf{Positive predicted value} (PPV) also called precision, is the rate that shows how often the model is correct when it predicts positive:
\bse
PPV = \frac{TP}{TP + FP}
\ese
\ed

\vspace{5pt}

\bd[False Discovery Rate]
\textbf{False discovery rate} (FDR) is the rate that shows how often the model is wrong when it predicts positive:
\bse
FDR = \frac{FP}{TP + FP}
\ese
\ed

\vspace{5pt}

It is of course: $PPV + FDR = 1$

\vspace{5pt}

\bd[Negative Predicted Value]
\textbf{Negative predicted value} (NPV) also called precision, is the rate that shows how often the model is correct when it predicts negative:
\bse
NPV = \frac{TN}{TN + FN}
\ese
\ed

\vspace{5pt}

\bd[False Omission Rate]
\textbf{False omission rate} (FOR) is the rate that shows how often the model is wrong when it predicts negative:
\bse
FOR = \frac{FN}{TN + FN}
\ese
\ed

\vspace{5pt}

It is of course: $NPV + FOR = 1$

\vspace{5pt}

\bd[$F_{\beta}$ Score]
\textbf{$F_{\beta}$ score} (FOR) is defined as the harmonic mean of positive predicted value PPV (aka precision) and true positive rate (aka recall) each weighted based on value of $\beta$:
\bse
F_{\beta} = \frac{(1 + \beta^2) \cdot PPV \cdot TPR}{\beta^2 \cdot PPV + TPR} =  \frac{(1 + \beta^2) \cdot \text{precission} \cdot \text{recall}}{\beta^2 \cdot \text{precission} + \text{recall}} = \frac{(1 + \beta^2) \cdot TP}{(1 + \beta^2) \cdot TP + \beta^2 \cdot FN + FP} 
\ese
\ed

\vspace{5pt}

The coefficient $\beta$ is chosen such that recall is considered $\beta$ times as important as precision. The most commonly used value for $\beta$ is 1, corresponding to the $F_{1}$ where precision and recall are weighted equally:
\bse
F_{1} = \frac{2 \cdot PPV \cdot TPR}{ PPV + TPR} =  \frac{2 \cdot \text{precission} \cdot \text{recall}}{\text{precission} + \text{recall}} = \frac{2 \cdot TP}{2 \cdot TP + FN + FP} 
\ese

\vspace{5pt}

Two other commonly used values for $\beta$ are 2 and 0.5, corresponding to the $F_{2}$ where weighs recall higher than precision (by placing more emphasis on false negatives) and the $F_{0.5}$ measure, which weighs recall lower than precision (by attenuating the influence of false negatives).

\bd[Null Error Rate]
\textbf{Null error rate} is the rate that shows how often a model would be wrong if it always predicts the most frequent type of outcome (either positive or negative depending on the dataset).
\ed

\bd[Cohen's Kappa]
\textbf{Cohen's kappa} is the rate that shows  how much better a model performs compared to a hypothetical model that would pick a category completely randomly.
\ed

\section{Support Vector Machine}

Support vector machine (SVM) is another, more advanced learning algorithm. It applies mainly in classification problems however there is also another model called support vector regression that applies the same ideas in regression problems. Here we will explore only SVM. \\

To tell the SVM story, we'll need to first talk about margins and the idea of separating data with a large ``gap". Next, we'll talk about the optimal margin classifier, which will lead us into a digression on Lagrange duality. We'll also see kernels, which give
a way to apply SVM's efficiently in very high dimensional (such as infinite dimensional) feature spaces, and finally, we'll close off the story with the SMO algorithm, which gives an efficient implementation of SVM's. \\

Consider logistic regression, where the probability $P(y = 1| \boldsymbol{x}; \boldsymbol{w})$ is modelled by 
\bse
h(\boldsymbol{w}^T \boldsymbol{x}) = \frac{1}{1 + exp(- \boldsymbol{w}^T \boldsymbol{x})}
\ese

We would then predict 1 on an input $\boldsymbol{x}$ if and only if $h(\boldsymbol{w}^T \boldsymbol{x}) \geq 0.5$ or equivalently, if and only if $\boldsymbol{w}^T \boldsymbol{x} \geq 0$. Consider a positive training example ($y = 1$). The larger $\boldsymbol{w}^T \boldsymbol{x}$ is, the larger also is $h(\boldsymbol{w}^T \boldsymbol{x})$ a.k.a the larger $P(y = 1| \boldsymbol{x}; \boldsymbol{w})$ is, and thus also the higher our degree of confidence that the label is 1. Thus, informally we can think of our prediction as being a very confident one that $y = 1$ if $\boldsymbol{w}^T \boldsymbol{x} \gg 0$. Similarly, we think of logistic regression as making a very confident prediction of $y = 0$, if $\boldsymbol{w}^T \boldsymbol{x} \ll 0$. Given a training set, again informally it seems that wed have found a good fit to the training data if we can find $\boldsymbol{w}$ so that $\boldsymbol{w}^T \boldsymbol{x}^{(i)} \gg 0$ whenever $y^(i) = 1$ and $\boldsymbol{w}^T \boldsymbol{x}^{(i)} \ll 0$ whenever $y^(i) = 0$, since this would reflect a very confident (and correct) set of classifications for all the training examples. This seems to be a nice goal to aim for ,and well soon formalize this idea using the notion of functional margins.\\

For a different type of intuition, consider the following figure, in which the symbol ``X" represent positive training examples, the symbol ``O" denote negative training examples, a decision boundary (this is the line given by the equation $\boldsymbol{w}^T \boldsymbol{x} = 0$ is also called the separating hyperplane) is also shown, and three points have also been labelled ``A", ``B" and ``C".

\begin{figure}[H]
\includegraphics[scale=0.6]{svm}
\centering
\end{figure}

Notice that the point ``A" is very far from the decision boundary. If we are
asked to make a prediction for the value of $y$ at ``A", it seems we should be
quite confident that $y = 1$ there. Conversely, the point ``C" is very close to
the decision boundary, and while it's on the side of the decision boundary
on which we would predict $y = 1$, it seems likely that just a small change to
the decision boundary could easily have caused out prediction to be $y = 0$.
Hence, we're much more confident about our prediction at ``A" than at ``C". The
point ``B" lies in-between these two cases, and more broadly, we see that if
a point is far from the separating hyperplane, then we may be significantly
more confident in our predictions. Again, informally we think it'd be nice if,
given a training set, we manage to find a decision boundary that allows us
to make all correct and confident (meaning far from the decision boundary)
predictions on the training examples. We'll formalize this later using the
notion of geometric margins. \\

To make our discussion of SVM's easier, we'll first need to introduce a new
notation for talking about classification. We will be considering a linear
classifier for a binary classification problem with labels $y$ and features $x$.
From now, we'll use $y \in \{ -1, 1 \}$ (instead of $ \{ 0, 1 \} $) to denote the class labels. Also, we will separate the $w_0$ component from $\boldsymbol{w}$ and from now on we will be denoting it $b$, and we will write our classifier as
\bse
h_{\boldsymbol{w}, b} (\boldsymbol{x}) = g(\boldsymbol{w}^T \boldsymbol{x} + b)
\ese

\vspace{3pt}

This $\boldsymbol{w}$, $b$ notation allows us to explicitly treat the intercept term $b$ separately from the other parameters. (We also drop the convention we had previously of letting $x_0 = 1$ be an extra coordinate in the input feature vector.) Note also that, from our definition of $g$ above, our classifier will directly predict either 1 or -1  without first going through the intermediate step of estimating the probability of $y$ being 1 (which was what logistic regression did).

\bd[Functional Margin Of A Training Example]
Given a training example $(x^{(i)}, y^{(i)})$ we define the \textbf{functional margin} of  $(\boldsymbol{w}$, $b$) with respect to the training example as
\bse
{\hat{\gamma}}^{(i)} = y^{(i)} (\boldsymbol{w}^T \boldsymbol{x}^{(i)} + b)
\ese
\ed

Note that if $y^{(i)}= 1,$ then for the functional margin to be large (i.e., for
our prediction to be confident and correct), we need $\boldsymbol{w}^T \boldsymbol{x}^{(i)} + b$ to be a large positive number. Conversely, if $y^{(i)}= -1,$ then for the functional margin to be large (i.e., for our prediction to be confident and correct), we need $\boldsymbol{w}^T \boldsymbol{x}^{(i)} + b$ to be a large negative number. Moreover, if $\boldsymbol{w}^T \boldsymbol{x}^{(i)} + b \neq 0$, then our prediction on this example is correct. Hence, a large functional margin represents a confident and a correct prediction. \\

For a linear classifier with the choice of $g$ given above, there's one property of the functional margin that makes it not a very good measure of confidence, however. Given our choice of $g$, we note that if we replace $\boldsymbol{w}$ with $2\boldsymbol{w}$ and $b$ with $2b$, then since $g(\boldsymbol{w}^T \boldsymbol{x} + b) = g(2\boldsymbol{w}^T \boldsymbol{x} + 2b)$ this would not change $h_{\boldsymbol{w}, b} (\boldsymbol{x})$ at all meaning that $g$, and hence also $h_{\boldsymbol{w}, b} (\boldsymbol{x})$, depends only on the sign, but not on the magnitude, of $\boldsymbol{w}^T \boldsymbol{x} + b$. However, replacing the scaling by a factor also results in multiplying our functional margin by the same factor. Thus, it seems that by exploiting our freedom to scale $\boldsymbol{w}$ and $b$, we can make the functional margin arbitrarily large without really changing anything meaningful. Intuitively, it might therefore make sense to impose some sort of normalization condition. \\

Given a training set we also define the functional margin of ($\boldsymbol{w}$, $b$) with respect to the set as
\bd[Functional Margin Of A Set]
Given a training set $\{ x^{(i)}, y^{(i)} \}$ we define the \textbf{functional margin} of  $(\boldsymbol{w}$, $b$) with respect to the training set as
\bse
{\hat{\gamma}} = \min {\hat{\gamma}}^{(i)}
\ese
\ed

Next, let's talk about geometric margins. Consider the picture below

\begin{figure}[H]
\includegraphics[scale=0.5]{svm2}
\centering
\end{figure}

The decision boundary corresponding to ($\boldsymbol{w}$, $b$) is shown, along with the vector $\boldsymbol{w}$. Note that $\boldsymbol{w}$ is orthogonal to the separating hyperplane. Consider the point at $A$, which represents the input $\boldsymbol{x}^{(i)}$ label $y^{(i)} = 1$. Its distance to the decision boundary, $\gamma^{(i)}$ is given by the line segment $AB$. \\

How can we find the value of $y^{(i)}$? Well, $\frac{\boldsymbol{w}}{||\boldsymbol{w}||}$ is a unit-length vector pointing in the same direction as $\boldsymbol{w}$. Since $A$ represents $\boldsymbol{x}^{(i)}$ we therefore find that the point $B$ is given by $\boldsymbol{x}^{(i)} - \gamma^{(i)} \frac{\boldsymbol{w}}{||\boldsymbol{w}||}$. But this point lies on the decision boundary, and all points on the decision boundary satisfy the equation $\boldsymbol{w}^T \boldsymbol{x} + b = 0$. Hence
\bse
\boldsymbol{w}^T \Big( \boldsymbol{x}^{(i)} - \gamma^{(i)} \frac{\boldsymbol{w}}{||\boldsymbol{w}||} \Big) + b = 0
\ese

Solving for $\gamma^{(i)}$ yields
\bse
\gamma^{(i)} = \frac{\boldsymbol{w}^T  \boldsymbol{x}^{(i)} + b }{||\boldsymbol{w}||} = \left( \frac{\boldsymbol{w}}{||\boldsymbol{w}||} \right)^T \boldsymbol{x}^{(i)} + \frac{b}{||\boldsymbol{w}||}
\ese

This was worked out for the case of a positive training example at $A$ in the
figure, where being on the positive side of the decision boundary is good.
More generally, we define the geometric margin of ($\boldsymbol{w}$, $b$) with respect to a training example $(x^{(i)}, y^{(i)})$ to be
\bse
\gamma^{(i)} =  y^{(i)} \left( \left( \frac{\boldsymbol{w}}{||\boldsymbol{w}||} \right)^T \boldsymbol{x}^{(i)} + \frac{b}{||\boldsymbol{w}||} \right)
\ese

Note that if $||\boldsymbol{w}|| = 1$, then the functional margin equals the geometric margin. This thus gives us a way of relating these two different notions of
margin. Also, the geometric margin is invariant to rescaling of the parameters. This will in fact come in handy later. Specifically, because of this invariance to the scaling of the parameters, when trying to fit $\boldsymbol{w}$  and $b$ to training data, we can impose an arbitrary scaling constraint on $\boldsymbol{w}$  without
changing anything important.\\

Finally, given a training set we also define the geometric margin of (w, b) with respect to the set to be the smallest of the geometric margins on the individual training examples:
\bse
\gamma = \min {\gamma}^{(i)}
\ese








