\babel@toc {english}{}
\contentsline {chapter}{\numberline {1}Deep Learning}{3}{chapter.1}%
\contentsline {section}{\numberline {1.1}McCulloch Pitts Neuron}{4}{section.1.1}%
\contentsline {section}{\numberline {1.2}Perceptron}{7}{section.1.2}%
\contentsline {section}{\numberline {1.3}Sigmoid Neuron}{10}{section.1.3}%
\contentsline {section}{\numberline {1.4}Feedforward Neural Networks}{11}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}Motivation: XOR Function With A Network Of Perceptons}{11}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {1.4.2}Forward Propagation}{14}{subsection.1.4.2}%
\contentsline {subsection}{\numberline {1.4.3}Backward Propagation}{16}{subsection.1.4.3}%
\contentsline {subsection}{\numberline {1.4.4}Extra: Generalize To Full Dataset}{18}{subsection.1.4.4}%
\contentsline {section}{\numberline {1.5}Modified Gradient Descent Algorithms}{20}{section.1.5}%
\contentsline {subsection}{\numberline {1.5.1}Mini-Batch \& Stochastic Gradient Descent}{20}{subsection.1.5.1}%
\contentsline {subsection}{\numberline {1.5.2}Gradient Descent With Momentum}{20}{subsection.1.5.2}%
\contentsline {subsection}{\numberline {1.5.3}Nesterov Accelerated Gradient Descent}{22}{subsection.1.5.3}%
\contentsline {subsection}{\numberline {1.5.4}Adaptive Gradient Descent (AdaGrad)}{22}{subsection.1.5.4}%
\contentsline {subsection}{\numberline {1.5.5}Root Mean Square Propagation (RMSProp)}{23}{subsection.1.5.5}%
\contentsline {subsection}{\numberline {1.5.6}Adaptive Moment Estimation (Adam)}{24}{subsection.1.5.6}%
\contentsline {subsection}{\numberline {1.5.7}Learning Rate Decay}{26}{subsection.1.5.7}%
\contentsline {section}{\numberline {1.6}Autoencoders}{27}{section.1.6}%
