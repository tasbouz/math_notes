\BOOKMARK [0][-]{chapter.1}{Deep Learning: Basics}{}% 1
\BOOKMARK [1][-]{section.1.1}{McCulloch Pitts Neuron}{chapter.1}% 2
\BOOKMARK [1][-]{section.1.2}{Perceptron}{chapter.1}% 3
\BOOKMARK [1][-]{section.1.3}{Sigmoid Neuron}{chapter.1}% 4
\BOOKMARK [1][-]{section.1.4}{Feedforward Neural Networks}{chapter.1}% 5
\BOOKMARK [2][-]{subsection.1.4.1}{Motivation: XOR Function With A Network Of Perceptons}{section.1.4}% 6
\BOOKMARK [2][-]{subsection.1.4.2}{Forward Propagation}{section.1.4}% 7
\BOOKMARK [2][-]{subsection.1.4.3}{Backward Propagation}{section.1.4}% 8
\BOOKMARK [2][-]{subsection.1.4.4}{Extra: Generalize To Full Dataset}{section.1.4}% 9
\BOOKMARK [1][-]{section.1.5}{Modified Gradient Descent Algorithms}{chapter.1}% 10
\BOOKMARK [2][-]{subsection.1.5.1}{Mini-Batch \046 Stochastic Gradient Descent}{section.1.5}% 11
\BOOKMARK [2][-]{subsection.1.5.2}{Gradient Descent With Momentum}{section.1.5}% 12
\BOOKMARK [2][-]{subsection.1.5.3}{Nesterov Accelerated Gradient Descent}{section.1.5}% 13
\BOOKMARK [2][-]{subsection.1.5.4}{Adaptive Gradient Descent \(AdaGrad\)}{section.1.5}% 14
\BOOKMARK [2][-]{subsection.1.5.5}{Root Mean Square Propagation \(RMSProp\)}{section.1.5}% 15
\BOOKMARK [2][-]{subsection.1.5.6}{Adaptive Moment Estimation \(Adam\)}{section.1.5}% 16
\BOOKMARK [2][-]{subsection.1.5.7}{Learning Rate Decay}{section.1.5}% 17
\BOOKMARK [1][-]{section.1.6}{Errors, Evaluation \046 Regularization}{chapter.1}% 18
\BOOKMARK [2][-]{subsection.1.6.1}{L2 Regularization}{section.1.6}% 19
\BOOKMARK [2][-]{subsection.1.6.2}{Dropout}{section.1.6}% 20
\BOOKMARK [2][-]{subsection.1.6.3}{Dataset Augmentation}{section.1.6}% 21
\BOOKMARK [2][-]{subsection.1.6.4}{Early Stopping}{section.1.6}% 22
\BOOKMARK [1][-]{section.1.7}{Improvements On A Feedforward Neural Network}{chapter.1}% 23
\BOOKMARK [2][-]{subsection.1.7.1}{Activation Functions}{section.1.7}% 24
\BOOKMARK [2][-]{subsection.1.7.2}{Initialization Strategies}{section.1.7}% 25
\BOOKMARK [2][-]{subsection.1.7.3}{Batch Normalization}{section.1.7}% 26
